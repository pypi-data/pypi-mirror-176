# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': '.', 'psycopg2': './psycopg2'}

packages = \
['psycopg2',
 'puff',
 'puff.contrib',
 'puff.contrib.django',
 'puff.contrib.django.postgres']

package_data = \
{'': ['*']}

install_requires = \
['asgiref>=3.5.2,<4.0.0', 'greenlet>=2.0.1,<3.0.0']

setup_kwargs = {
    'name': 'puff-py',
    'version': '0.1.9',
    'description': 'Python support for Puff',
    'long_description': '# â˜ Puff â˜\n\nPython with an async runtime built-in Rust for GraphQL, ASGI, WSGI, Postgres, PubSub, Redis, Distributed Tasks, and HTTP2 Client.\n\n[![Crates.io][crates-badge]][crates-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Documentation](https://docs.rs/puff-rs/badge.svg)](https://docs.rs/puff_rs)\n\n[crates-badge]: https://img.shields.io/crates/v/puff_rs.svg\n[crates-url]: https://crates.io/crates/puff_rs\n[mit-badge]: https://img.shields.io/badge/license-MIT-blue.svg\n[mit-url]: https://github.com/hansonkd/puff/blob/master/LICENSE\n\n- [What is Puff?](#what-is-puff)\n  * [Quick Start](#quick-start)\n  * [Puff â™¥ Python](#puff--python)\n  * [Puff â™¥ Django](#puff--django)\n  * [Puff â™¥ Graphql](#puff--graphql)\n  * [Puff â™¥ Pytest](#puff--pytest)\n  * [Puff â™¥ AsyncIO](#puff--asyncio)\n  * [Puff â™¥ Django + GraphQL](#puff--django--graphql)\n  * [Puff â™¥ Distributed Tasks](#puff--distributed-tasks)\n  * [Puff â™¥ HTTP](#puff--http)\n  * Links\n      - [CHANGELOG](https://github.com/hansonkd/puff/blob/master/CHANGELOG.md)\n      - [Deepstack](https://github.com/hansonkd/puff/blob/master/book/Deepstack.md)\n      - [Using the CLI](https://github.com/hansonkd/puff/blob/master/book/CLI.md)\n      - [Flask](https://github.com/hansonkd/puff/blob/master/book/Flask.md)\n      - [FastAPI](https://github.com/hansonkd/puff/blob/master/book/FastAPI.md)\n      - [Django](https://github.com/hansonkd/puff/blob/master/book/Django.md)\n      - [Building an RPC with Puff](https://github.com/hansonkd/puff/blob/master/book/RPC.md)\n\n# What is Puff?\n\nPuff is a batteries included "deep stack" for Python. It\'s an experiment to minimize the barrier between Python and Rust to unlock the full potential of high level languages. Build your own Runtime using standard CPython and extend it with Rust. Imagine if GraphQL, Postgres, Redis and PubSu, Distributed Tasks were part of the standard library. That\'s Puff.\n\nThe old approach for integrating Rust in Python would be to make a Python package that uses rust and import it from Python. This approach has some flaws as the rust packages can\'t cooperate. Puff gives Rust its own layer, so you can build a cohesive set of tools in Rust that all work flawlessly together without having to re-enter Python.\n\nHigh level overview is that Puff gives Python\n\n* Greenlets on Rust\'s Tokio.\n* High performance HTTP Server - combine Axum with Python WSGI apps (Flask, Django, etc.)\n* Rust / Python natively in the same process, no sockets or serialization.\n* AsyncIO / uvloop / ASGI integration with Rust\n* An easy-to-use GraphQL service\n* Multi-node pub-sub\n* Rust level Redis Pool\n* Rust level Postgres Pool\n* Websockets\n* HTTP Client\n* Distributed, at-least-once, priority and scheduled task queue\n* semi-compatible with Psycopg2 (hopefully good enough for most of Django)\n* A safe convenient way to drop into rust for maximum performance\n\nThe idea is Rust and Python are near perfect complements to each other and building a framework to let them talk\nleads to greater efficiency in terms of productivity, scalability and performance.\n\n| Python                                             | Rust                                            |\n|----------------------------------------------------|-------------------------------------------------|\n| âœ… High-Level                                       | âœ… Low-Level                                     |\n| âœ… Lots of tools and packages                       | âœ… Lots of tools and packages                    |\n| âœ… Easy to get started                              | âœ… Easy to get started                           |\n| ðŸŸ¡ Interpreted (productivity at the cost of speed) | ðŸŸ¡ Compiled (speed at the cost of productivity) |\n| âœ… Easy to get master                               | âŒ The learning curve gets steep quickly.        |\n| âœ… Fast iteration to prototype                      | âŒ Requires planning for correctness             |\n| âœ… Google a problem, copy paste, it works.          | âŒ Less examples floating in the wild            |\n| âŒ Weak type system                                 | âœ… Great Type System                             |\n| âŒ GIL prevents threading                           | âœ… High Performance                              |\n| âŒ Not-so safe                                      | âœ… Safe                                          |\n\n\nThe Zen of deepstack is recognizing that no language is the ultimate answer. Seek progress instead of perfection by using Python for rapid development and Rust to optimize the most critical paths once you find them later. Find the balance. \n\n## Quick Start\n\nInstall Rust to compile Puff for your platform.\n\n#### Install Rust\n\nFollow the [instructions](https://www.rust-lang.org/tools/install) to install Cargo for your platform.\n\n```bash\ncurl --proto \'=https\' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n#### Install Puff\n\nUse cargo to install Puff.\n\n```bash\ncargo install puff-rs\n```\n\nPuff requires Python >= 3.10. Python\'s [Poetry](https://python-poetry.org) is optional.\n\nYour Puff Project needs to find your Python project. Even if they are in the same folder, they need to be added to the PYTHONPATH.\n\nOne way to set up a Puff project is like this:\n\n```bash\npoetry new my_puff_proj_py\ncd my_puff_proj_py\npoetry add puff-py\n```\n\nNow from `my_puff_proj_py` you can run your project with `poetry run puff` to access cargo from poetry and expose the virtual environment to Puff.\n\nThe Python project doesn\'t need to be inside off your Rust package. It only needs to be on the PYTHONPATH or inside an virtualenv. If you don\'t want to use poetry, you will have to set up a virtual environment when running Puff.\n\n\n## Puff â™¥ Python\n\nPython programs in Puff are run by building a `Program` in Rust and registering the Python function there.\n\nThe Python method is bootstrapped and run as a greenlet in the Puff runtime.\n\nCreate a `puff.toml`\n\n```toml title="puff.toml"\n[[commands]]\nfunction = "my_puff_proj_py.hello_world"\ncommand_name = "hello_world"\n```\n\nPython: \n\n```python title="/app/my_puff_proj_py/__init__.py"\nimport puff\n\n# Standard python functions run on Puff greenlets. You can only use special Puff async functions without pausing other greenlets.\ndef hello_world():\n    fn = "my_file.zip"\n    result_bytes = puff.read_file_bytes(fn) # Puff async function that runs in Tokio.\n    result_py_bytes = do_some_blocking_work(fn) # Python blocking that spawns a thread to prevent pausing the greenlet thread.\n    print(f"Hello from python!! Zip is {len(result_bytes)} bytes long from rust and {len(result_py_bytes)} bytes from Python.")\n\n\n# 100% of python packages are compatible by wrapping them in blocking decorator.\n@puff.blocking    \ndef do_some_blocking_work(fn):\n    with open(fn, "rb") as f:\n        return f.read()\n```\n\n\n## Puff â™¥ Django\n\nWhile it can run any WSGI app, Puff has a special affection for Django. Puff believes that business logic should be implemented on a higher level layer and Rust should be used as an optimization. Django is a perfect high level framework to use with Puff as it handles migrations, admin, etc. Puff mimics the psycopg2 drivers and cache so that Django uses the Puff Database and Redis pool.\n\nTransform your sync Django project into a highly concurrent Puff program with a few lines of code. Puff wraps the management commands so migrate, etc. all work as expected. Simply run `poetry run run_cargo django [command]` instead of using `./manage.py [command]`. For example `poetry run run_cargo django migrate`. Don\'t use django\'s dev server, instead use Puff\'s with `poetry run run_cargo serve`.\n\n\nCreate a `puff.toml`\n\n```toml title="puff.toml"\ndjango = true\nwsgi = "my_django_application.wsgi.application"\n\n[[postgres]]\nname = "default"\n\n[[redis]]\nname = "default"\n```\n\nUse Puff everywhere in your Django app. Even create Django management commands that use Rust!\n\nSee [puff-py repo](https://github.com/hansonkd/puff-py) for a more complete Django example.\n\n\n## Puff â™¥ Graphql\n\nPuff exposes Graphql Mutations, Queries and Subscriptions based on Python Class definitions. A core "killer feature" of the Puff Graphql engine is that it works on a "layer base" instead of a Node base. This allows each step of Graphql to gather the complete data necessary to query all data it needs at once. This avoids the dreaded n+1 and dataloader overhead traditionally associated with GraphQL.\n\nGrapqhQL python functions can pass off Pure SQL queries to Puff and puff will render and transform the query without needing to return to python. This allows the Python Graphql interface to be largely IO free, but still flexible to have access to Puff resources when needed.\n\n```python title="/app/py_code/my_python_gql_app.py"\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, List, Any\nfrom puff.pubsub import global_pubsub\n\npubsub = global_pubsub\nCHANNEL = "my_puff_chat_channel"\n\n@dataclass\nclass SomeInputObject:\n    some_count: int\n    some_string: str\n\n\n@dataclass\nclass SomeObject:\n    field1: int\n    field2: str\n    \n@dataclass\nclass DbObject:\n    was_input: int\n    title: str\n    \n    @classmethod\n    def child_sub_query(cls, context, /) -> Tuple[DbObject, str, List[Any], List[str], List[str]]:\n        # Extract column values from the previous layer to use in this one.\n        parent_values = [r[0] for r in context.parent_values(["field1"])]\n        sql_q = "SELECT a::int as was_input, $2 as title FROM unnest($1::int[]) a"\n        # returning a sql query along with 2 lists allow you to correlate and join the parent with the child.\n        return ..., sql_q, [parent_values, "from child"], ["field1"], ["was_input"]\n\n\n@dataclass\nclass Query:\n\n    @classmethod\n    def hello_world(cls, parents, context, /, my_input: int) -> Tuple[List[DbObject], str, List[Any]]:\n        # Return a Raw query for Puff to execute in Postgres.\n        # The ellipsis is a placeholder allowing the Python type system to know which Field type it should transform into.\n        return ..., "SELECT $1::int as was_input, \\\'hi from pg\\\'::TEXT as title", [my_input]\n\n    @classmethod\n    def hello_world_object(cls, parents, context, /, my_input: List[SomeInputObject]) -> Tuple[List[SomeObject], List[SomeObject]]:\n        objs = [SomeObject(field1=0, field2="Python object")]\n        if my_input:\n            for inp in my_input:\n                objs.append(SomeObject(field1=inp.some_count, field2=inp.some_string))\n        # Return some normal Python objects.\n        return ..., objs\n    \n    @classmethod\n    def new_connection_id(cls, context, /) -> str:\n        # Get a new connection identifier for pubsub.\n        return pubsub.new_connection_id()\n\n\n@dataclass\nclass Mutation:\n    @classmethod\n    def send_message_to_channel(cls, context, /, connection_id: str, message: str) -> bool:\n        print(context.auth_token) #  Authoritzation bearer token passed in the context\n        return pubsub.publish_as(connection_id, CHANNEL, message)\n\n\n@dataclass\nclass MessageObject:\n    message_text: str\n    from_connection_id: str\n    num_processed: int\n\n\n@dataclass\nclass Subscription:\n    @classmethod\n    def read_messages_from_channel(cls, context, /, connection_id: Optional[str] = None) -> Iterable[MessageObject]:\n        if connection_id is not None:\n            conn = pubsub.connection_with_id(connection_id)\n        else:\n            conn = pubsub.connection()\n        conn.subscribe(CHANNEL)\n        num_processed = 0\n        while msg := conn.receive():\n            from_connection_id = msg.from_connection_id\n            # Filter out messages from yourself.\n            if connection_id != from_connection_id:\n                yield MessageObject(message_text=msg.text, from_connection_id=from_connection_id, num_processed=num_processed)\n                num_processed += 1\n\n\n@dataclass\nclass Schema:\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n\n```\n\nRust:\n\n```toml title="puff.toml"\ndjango = true\npytest = true\n\n[[postgres]]\nenable = true\n\n[[redis]]\nenable = true\n\n[[pubsub]]\nenable = true\n\n[[graphql]]\nschema = "my_python_gql_app.Schema"\nurl = "/graphql/"\nsubscriptions_url = "/subscriptions/"\nplayground_url = "/playground/"\n\n[[commands]]\nfunction = "my_puff_proj_py.hello_world"\ncommand_name = "hello_world"\n```\n\nProduces a Graphql Schema like so:\n\n![Schema](https://user-images.githubusercontent.com/496914/195461156-1613c3e6-7b82-4143-8796-1b95ff10f7c3.png)\n\nIn addition to making it easier to write the fastest queries, a layer based design allows Puff to fully exploit the multithreaded async Rust runtime and solve branches independently. This gives you a  performance advantages out of the box.\n\n## Puff â™¥ Pytest\n\nIntegrate with pytest to easily test your Graphql and Puff apps. Simply add the `PytestCommand` to your Program and write tests as normal only run them with `puff pytest`\n\n```toml title="puff.toml"\npytest = true\n```\n\n```python title="/app/src/py_code/test_gql.py"\nfrom hello_world_py_app import __version__\nfrom puff.graphql import global_graphql\n\ngql = global_graphql\n\n\ndef test_version():\n    assert __version__ == \'0.1.0\'\n\n\ndef test_gql():\n    QUERY = """\n    query {\n        hello_world(my_input: 3) {\n            title\n            was_input\n        }\n    }\n    """\n    result = gql.query(QUERY, {})\n    assert \'data\' in result\n    assert \'errors\' not in result\n    assert result[\'data\']["hello_world"][0]["title"] == "hi from pg"\n    assert result[\'data\']["hello_world"][0]["was_input"] == 3\n```\n\n## Puff â™¥ AsyncIO\n\nPuff has built in integrations for ASGI and asyncio. You first need to configure the RuntimeConfig to use it. Puff will automatically use uvloop if installed when starting the event loop.\n\n`asgiref.sync.async_to_sync` and `asgiref.sync.sync_to_async` have both been patched so that you can call puff greenlets from async or async from puff greenlets easily.\n\n```python title="/app/src/py_code/fast_api_example.py"\nfrom fastapi import FastAPI\nfrom puff import global_state, wrap_async\n\n\nstate = global_state\n\napp = FastAPI()\n\n\n@app.get("/fast-api")\nasync def read_root():\n    result = await wrap_async(lambda r: state.hello_from_rust_async(r, "hello from asyncio"))\n    return {"Hello": "World", "from": "Fast API", "rust_value": result}\n```\n\n`puff.toml`\n\n```toml title="puff.toml"\nasyncio = true\nasgi = "my_python_app.app"\n```\n\n## Puff â™¥ Django + Graphql\n\nPuff GraphQL integrates seamlessly with Django. Convert Django querysets to SQL to offload all computation to Rust. Or decorate with `borrow_db_context` and let Django have access to the GraphQL connection, allowing you fallback to the robustness of django for complicated lookups.\n\n```python\nfrom dataclasses import dataclass\nfrom puff import graphql\nfrom polls.models import Question, Choice\nfrom django.utils import timezone\nfrom  puff.contrib.django import query_and_params\n\n\n@dataclass\nclass ChoiceObject:\n    id: int\n    question_id: int\n    choice_text: str\n    votes: int\n\n\n@dataclass\nclass QuestionObject:\n    id: int\n    pub_date: str\n    question_text: str\n\n    @classmethod\n    def choices(cls, context, /) -> Tuple[List[ChoiceObject], str, List[Any], List[str], List[str]]:\n        # Extract column values from the previous layer to use in this one.\n        parent_values = [r[0] for r in context.parent_values(["id"])]\n        # Convert a Django queryset to sql and params to pass off to Puff. This function does 0 IO in Python.\n        qs = Choice.objects.filter(question_id__in=parent_values)\n        sql_q, params = query_and_params(qs)\n        return ..., sql_q, params, ["id"], ["question_id"]\n\n\n@dataclass\nclass Query:\n\n    @classmethod\n    def questions(cls, context, /) -> Tuple[List[QuestionObject], str, List[Any]]:\n        # Convert a Django queryset to sql and params to pass off to Puff. This function does 0 IO in Python.\n        qs = Question.objects.all()\n        sql_q, params = query_and_params(qs)\n        return ..., sql_q, params\n\n    @classmethod\n    @graphql.borrow_db_context  # Decorate with borrow_db_context to use same DB connection in Django as the rest of GQL\n    def question_objs(cls, context, /) -> Tuple[List[QuestionObject], List[Any]]:\n        # You can also compute the python values with Django and hand them off to Puff.\n        # This version of the same `questions` field, is slower since Django is constructing the objects.\n        objs = list(Question.objects.all())\n        return ..., objs\n\n\n@dataclass\nclass Mutation:\n    @classmethod\n    @graphql.borrow_db_context  # Decorate with borrow_db_context to use same DB connection in Django as the rest of GQL\n    def create_question(cls, context, /, question_text: str) -> QuestionObject:\n        question = Question.objects.create(question_text=question_text, pub_date=timezone.now())\n        return question\n\n@dataclass\nclass Subscription:\n    pass\n\n@dataclass\nclass Schema:\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n\n```\n\n## Puff â™¥ Distributed Tasks\n\nSometimes you need to execute a function in the future, or you need to execute it, but you don\'t care about the result right now. For example, you might have a webhook or an email to send.\n\nPuff provides a distributed queue abstraction as part of the standard library. It is powered by Redis and has the ability to distribute tasks across nodes with priorities, delays and retries. Jobs submitted to the queue can be persisted (additionally so if Redis is configured to persist to disk), so you can shut down and restart your server without worrying about losing your queued functions.\n\nThe distributed queue runs in the background of every Puff instance. In order to have a worker instance, use the `WaitForever` command. Your HTTP server can also handle distributing, processing and running background tasks which is handy for small projects and scales out well by using `wait_forever` to add more processing power if needed.\n\nA task is a python function that takes a JSONable payload that you care executes, but you don\'t care exactly when or where. JSONable types are simple Python structures (dicts, lists, strings, etc) that can be serialized to JSON. Queues will monitor tasks and retry them if they don\'t get a result in `timeout_ms`. Beware that you might have the same task running multiple times if you don\'t configure timeouts correctly, so if you are sending HTTP requests or other task that might take a while to respond configure timeouts correctly. Tasks should return a JSONable result which will be kept for `keep_results_for_ms` seconds.\n\nOnly pass in top-level functions into `schedule_function` that can be imported (no lambda\'s or closures). This function should be accessible on all Puff instances.\n\nImplement priorities by utilizing `scheduled_time_unix_ms`. The worker sorts all tasks by this value and executes the first one up until the current time. So if you schedule `scheduled_time_unix_ms=1`, that function will be the next to execute on the first availability. Use `scheduled_time_unix_ms=1`, `scheduled_time_unix_ms=2`. `scheduled_time_unix_ms=3`, etc for different task types that are high priority. Be careful that you don\'t starve the other tasks if you aren\'t processing these high priority tasks fast enough. By default, Puff schedules new tasks with the current unix time to be "fair" and provide a sense of "FIFO" order. You can also set this value to a unix timestamp in the future to delay execution of a task.\n\nYou can have as many tasks running as you want (use `set_task_queue_concurrent_tasks`), however there is a small overhead in terms of monitoring and finding new tasks by increasing this value. The default is `num_cpu x 4`\n\nSee additional design patterns in [Building RPC with Puff](https://github.com/hansonkd/puff/blob/master/book/RPC.md).\n\n```python title="/app/src/py_code/task_queue_example.py"\nfrom puff.task_queue import global_task_queue\n\ntask_queue = global_task_queue\n\n\ndef run_main():\n    all_tasks = []\n    for x in range(100):\n        #  Schedule some tasks on any coroutine thread of any Puff instance connected through Redis.\n        task1 = task_queue.schedule_function(my_awesome_task, {"type": "coroutine", "x": [x]}, timeout_ms=100, keep_results_for_ms=5 * 1000)\n        #  Override `scheduled_time_unix_ms` so that async tasks execute with priority over the coroutine tasks.\n        #  Notice that since all of these tasks have the same priority, they may be executed out of the order they were scheduled.\n        task2 = task_queue.schedule_function(my_awesome_task_async, {"type": "async", "x": [x]}, scheduled_time_unix_ms=1)\n        #  These tasks will keep their order since their priorities as defined by `scheduled_time_unix_ms` match the order scheduled.\n        task3 = task_queue.schedule_function(my_awesome_task_async, {"type": "async-ordered", "x": [x]}, scheduled_time_unix_ms=x)\n        print(f"Put tasks {task1}, {task2}, {task3} in queue")\n        all_tasks.append(task1)\n        all_tasks.append(task2)\n        all_tasks.append(task3)\n\n    for task in all_tasks:\n        result = task_queue.wait_for_task_result(task, 100, 1000)\n        print(f"{task} returned {result}")\n\n\ndef my_awesome_task(payload):\n    print(f"In task {payload}")\n    return payload["x"][0]\n\n\nasync def my_awesome_task_async(payload):\n    print(f"In async task {payload}")\n    return payload["x"][0]\n```\n\n`puff.toml`\n\n```toml title="puff.toml"\nasyncio = true\n\n[[task_queue]]\nenable = true\n```\n\n## Puff â™¥ HTTP\n\nPuff has a built-in asynchronous HTTP client based on reqwests that can handle HTTP2 (also served by the Puff WSGI/ASGI integrations) and reuse connections. It uses rust to encode and decode JSON ultra-fast.\n\n```python\nfrom puff.http import global_http_client\n\nhttp_client = global_http_client\n\n\nasync def do_http_request():\n    this_response = await http_client.post("http://localhost:7777/", json={"my_data": ["some", "json_data"]})\n    return await this_response.json()\n\n\ndef do_http_request_greenlet():\n    """greenlets can use the same async functions. Puff will automatically handle awaiting and context switching."""\n    this_response = http_client.post("http://localhost:7777/", json={"my_data": ["some", "json_data"]})\n    return this_response.json()\n```\n\nYou can set the HTTP client options through RuntimeConfig. If your program is only talking to other Puff instances or HTTP2 services, it can make sense to turn on HTTP2 only. You can also configure user-agents as well as many other HTTP options through this method.\n\n```toml title="puff.toml"\nasyncio = true\n\n[[http_client]]\nhttp2_prior_knowledge = true\n```\n\n## Connect to Everything...\n\nPuff supports multiple pools to services.\n\n```toml title="puff.toml"\n[[postgres]]\nname = "default"\n\n[[postgres]]\nname = "readonly"\n\n[[postgres]]\nname = "audit"\n\n[[redis]]\nname = "default"\n\n[[redis]]\nname = "other"\n\n[[http_client]]\nname = "default"\n\n[[http_client]]\nname = "internal"\nhttp2_prior_knowledge = true\n\n[[pubsub]]\nname = "default"\n\n[[pubsub]]\nname = "otherpubsub"\n\n[[graphql]]\nschema = "my_python_gql_app.Schema"\nurl = "/graphql/"\nsubscriptions_url = "/subscriptions/"\nplayground_url = "/playground/"\ndatabase = "readonly"\n\n[[graphql]]\nname = "audit"\nschema = "my_python_gql_app.AuditSchema"\nurl = "/audit/graphql/"\nsubscriptions_url = "/audit/subscriptions/"\nplayground_url = "/audit/playground/"\ndatabase = "audit"\n```\n\nProduces a Program with the following options:\n\n```bash\nOptions:\n      --default-postgres-url <DEFAULT_POSTGRES_URL>\n          Postgres pool configuration for \'default\'. [env: PUFF_DEFAULT_POSTGRES_URL=] [default: postgres://postgres:password@localhost:5432/postgres]\n      --audit-postgres-url <AUDIT_POSTGRES_URL>\n          Postgres pool configuration for \'audit\'. [env: PUFF_AUDIT_POSTGRES_URL=] [default: postgres://postgres:password@localhost:5432/postgres]\n      --readonly-postgres-url <READONLY_POSTGRES_URL>\n          Postgres pool configuration for \'readonly\'. [env: PUFF_READONLY_POSTGRES_URL=] [default: postgres://postgres:password@localhost:5432/postgres]\n      --default-redis-url <DEFAULT_REDIS_URL>\n          Redis pool configuration for \'default\'. [env: PUFF_DEFAULT_REDIS_URL=] [default: redis://localhost:6379]\n      --other-redis-url <OTHER_REDIS_URL>\n          Redis pool configuration for \'other\'. [env: PUFF_OTHER_REDIS_URL=] [default: redis://localhost:6379]\n      --default-pubsub-url <DEFAULT_PUBSUB_URL>\n          PubSub configuration for \'default\'. [env: PUFF_DEFAULT_PUBSUB_URL=] [default: redis://localhost:6379]\n      --otherpubsub-pubsub-url <OTHERPUBSUB_PUBSUB_URL>\n          PubSub configuration for \'otherpubsub\'. [env: PUFF_OTHERPUBSUB_PUBSUB_URL=] [default: redis://localhost:6379]\n```\n\n\n## Deepstack\n\nPuff is designed so that you can build your own version using Puff as a library. This allows an incredible depth of performance optimization if necessary for your project.\n\n## Architecture\n\nPuff consists of multithreaded Tokio Runtime and a single thread which runs all Python computations on Greenlets. Python offloads the IO to Tokio which schedules it and returns it if necessary.\n\n![Untitled Diagram-2](https://user-images.githubusercontent.com/496914/195153405-7a1c7bcf-a864-4502-806c-c7d5e7aac3b9.png)\n\n\n## Status\n\nThis is extremely early in development. The scope of the project is ambitious. Expect things to break. \n\nProbably the end game of puff is to have something like gevent\'s monkeypatch to automatically make existing projects compatible.',
    'author': 'Kyle Hanson',
    'author_email': 'me@khanson.io',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/hansonkd/puff',
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.10,<4.0',
}


setup(**setup_kwargs)
