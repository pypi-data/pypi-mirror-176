---
title: "Generative Models and Time Series Predictions for Financial Applications with Kernels Methods"
author: Jean-Marc Mercier $^\ddag$ \footnote{MPG-Partners, 136 Boulevard Haussmann, 75008 Paris, France. jean-marc.mercier@mpg-partners.com}
date: "`r format(Sys.time(), '%d %m %Y')`"
abstract: This presentation deals with generative and predictive models based on kernel methods. After introducing our kernel library, We illustrate these models with a toy-example of risk framework based on time series forecasting, that could be used for real-time P\&L explanation of large derivative portfolios, or other risk measures as Expected Positive Exposure or Credit Valuation Risk. 
output:
  pdf_document:
    keep_tex: yes
    includes:
      in_header: article_sty.sty
    number_sections: yes
  html_document: default
  word_document: default
---

```{r include=FALSE, echo = FALSE, code=xfun::read_utf8('preamble.R')}
```


```{python, results = 'hide'}
from pres import *
```


# Introduction 

In this jupyter notebook presentation, we would like to introduce our internal AI library, highlighting it with two interesting applications of machine learning to finance, namely:

\begin{itemize}
\item Synthetic data generation. For finance applications, there exists numerous applications, among them are time series forecast of risk sources that we illustrate in this presentation. The most-known technologies for producing synthetic datas are neural networks based, as for instance GAN, WGAN, CLGAN, that are at heart of synthetic images or videos production.
\item Predictives methods. Here too, applications for finance are numerous, and we illustrate here an application to real-time P\&L computations. The most-known technologies for predictive methods are for instance neural networks, decision trees, etc...
\end{itemize}
For this presentation, we used our open source library, codpy \footnote{The codpy user manual is accessible 
\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4077158}{clicking here}. For installation, follow the guidelines \href{https://pypi.org/project/codpy/}{clicking here}.}. Codpy is a kernel based technology (RHKS - Reproducing Kernel Hilbert Space theory), that we have developped and are using for the internal needs at MPG-Partners. Codpy is an alternative library to other AI libraries (pytorch, theano, tensorflow, etc...) that has some nice properties for finance applications:

\begin{itemize}
\item It is an explainable method. All produced results come with computable error estimates allowing to qualify them. Error estimates allow moreover to link naturally to Optimal Transport Theory based tools, used thoroughly by this library.  
\item It is a performing and accurate library, particularly while dealing with sparse input data (small training set).
\item It can compute efficiently a wide zoo of differential operators. Indeed, this library was thought primarily to solve mathematical physics based problems, and is used to approximate some dynamical systems described by a PDE (Partial Differential Equations) model.
\end{itemize}

\newpage

This presentation is structured as follows :

\tableofcontents

\newpage

# A quick tour to Artificial Intelligence - Five CoDpy python methods


## Predictive methods  

### Training sets and test sets


Artificial Intelligence (AI) is mainly based over predictions, that are nothing else but extrapolation methods. 

\textbf{AI Vocabulary:}. $X,f(X)$ is the \textbf{training set}. $Z$ is the \textbf{test set}, $f_z$ is the \textbf{prediction set}, $f(Z)$ is the reference (ground truth) value. $Y$ is the \textbf{parameter set}, that are internal parameters to a prediction algorithm. 


A simple example is given below : a collection of 100 one dimensional points $X$ lying on the segment [-1,1], together with their values given by a sinusoidal function $f(X)$. The test set are points on the segment [-1.5,1.5], allowing to check extrapolations behaviors.

```{python, label= xfxzfz, fig.cap="training set, test set and ground truth values for a one dimensional example."}
data_random_generator_ = data_random_generator(fun = my_fun,types=["cart","sto","cart"])
x, fx, y, fy, z, fz =  data_random_generator_.get_data(D=1,Nx=100,Ny=100,Nz=100)
multi_plot([(x, fx),(z, fz)],plot1D)
```

Another example of more elaborate training set : the MNIST database, composed of $60,000$ images defining a training set of handwritten digits. Each image is a vector having dimensions $784$ (a $28 \times 28$ grayscale image flattened in row-order), with their labels, $0–9$. The test set is composed of $10,000$ images.


```{python,label= mnist,fig.cap = "\\label{mnist} MNIST - distribution of 60 000 hand-written digits", results = 'hide'}
mnist = kernel_image_generator()
imshow(mnist['imx'], cmap='gray')
```


\newpage

### Predictive methods


There exists a lot of predictive machines or algorithms. To unify them, a predictor, denoted by $\mathcal{P}_m$, can be described as an extrapolation or interpolation operator, that defines a prediction as follows:
\begin{equation}\label{eq:Pm}
f_Z = \mathcal{P}_{m}(X,Y=[],Z=X,f(X)).
\end{equation}
$f_Z$, the prediction, has to be compared to $f(Z)$, the ground truth values.

Codpy provides facilities to compare methods, as illustrated in these figures, retrieved from our user manual for the one-dimensional problem.


```{python , label = list_of_list_results, results='hide', fig.cap="Example of predictions : Periodic kernel:CodPy, the RBF kernel: SciPy, SVR: Scikit, Neural Network: TensorFlow, Decision tree: Scikit, Adaboost: Scikit, XGBoost, Random Forest: Scikit"}
from book_funs2 import list_of_list
list_of_list_results = list_of_list()
title_list = ["Periodic kernel", "The RFB kernel", "SVR:Scikit", "NN","Decision tree", "Adaboost", "XGBoost", "RF"]
multi_plot(list_of_list_results,plot1D,mp_ncols = 4, f_names=title_list, mp_max_items = 8)
```

\newpage

### Benchmark of predictive methods

Once a prediction $f_Z$ is made, one want to compare to the ground truth values $f(Z)$, using some metrics, here a confusion matrix for the MNIST problem for codpy prediction method

```{python, label= Confusion,  fig.cap="Confusion matrix for codpy: Classifier", results = "hide"}
from book_funs5 import MNIST
scenario_generator_, mnist_list = MNIST()
multi_plot([scenario_generator_.predictor],add_confusion_matrix.plot_confusion_matrix,mp_title='codpy : MNIST confusion matrix')
```

For classification problems as MNIST, scores is a normalized metric (the higher the better) :
$$
  0 \le \frac{\# \{f_Z == f(Z)\}}{N_Z} \le 1
$$

Below are scores for the MNIST for several projection methods for different settings, corresponding to varying training set sizes.

```{python,  label = Scores, fig.cap="Scores and execution time", results = "hide"}
kwargs = {"mp_max_items" :4, "mp_ncols" : 4 }
scenario_generator_.compare_plots(
axis_field_labels = [("Nx","scores"),("Ny","execution_time")],**kwargs)
```


\newpage

### Kernel projection operator

Kernel methods define a quite simple prediction operator, that we present now briefly. Let $X$ (resp. $Y,Z$) be a set of $N_x$ (resp. $N_y,N_Z$) **distinct** points in dimension $D$, $f$ any continuous, vector valued, function. Notations
\begin{equation}\label{X}
X :=\{x^n_d\}_{n,d=1}^{N_x,D} \in \RR^{N_x,D}, \quad f(X) \in \RR^{N_x,D_f}
\end{equation}
Let $k$ be a kernel, that is a symmetric and positive definite (see \cite{BTA} for a definition) scalar function $k: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$.
Consider $K(X,Y)$ the Gram matrix, i.e. $K(X,Y):=(k(x^n,y^m))_{n,m = 1}^{N_x,N_y}$. We define a \textbf{prediction} as
\begin{equation} \label{projection}
f_Z := \mathcal{P}_{k}(X,Y,Z)f(X), \quad \mathcal{P}_{k}(X,Y,Z) := K(Y,Z) K(X,Y)^{-1}.
\end{equation}
The inverse is computed using a least-squares approach, as follows, $K(X,Y)^{-1} = ( K(Y,X)K(X,Y)+ \epsilon)^{-1}K(Y,X)$, where $\epsilon$ is a (optional) regularization term, as is the Tychonov regularization method, or other ones. A classical choice for the parameter set are $Y=X$ (extrapolation), or $Y \subset X$ (interpolation - Nystrom method). Starting from the formula \eqref{projection}, we can define all kind of differential operators, as for instance the gradient
\begin{equation} \label{grad}
\nabla f_Z = (\nabla_Z K)(Y,Z) K(X,Y)^{-1} f(X).
\end{equation}
CoDpy function:
$$
f_z = \text{ op.projection}(X,Y,Z,f(X), k,...),\nabla f_Z = \text{op.nabla}(X,Y,Z,f(X), k,...)
$$.

\newpage

## Error estimates with kernels : Dicrepancies and norms


The projection operator \eqref{projection} benefits from the following error estimate (see \cite{PLF-JMM-estimate}), that are  confidence levels
\begin{equation} \label{error}
\| f(Z) - f_Z \|_{\ell^2} \le D_k(X,Y,Z) \| f \|_{\mathcal{H}_k},
\end{equation}
where $D_k(X,Y,Z) := D_k(X,Y)+D_k(Y,Z)$, and where the discrepancy (called also MMD - Maximum Mean Discrepancy) between two discrete probability measures $X$ and $Y$, induced by a kernel $k$ is
\begin{equation}\label{Dk}
    D_k\big(X,Y\big)^2 := \frac{ \sum_{n,m=1}^{N_x}  k(x^n,x^m)}{N_x^2} + \frac{\sum_{n,m}^{N_y} k(y^n,y^m)}{N_y^2} - \frac{2 \sum_{n,m=1}^{N_x,N_y} k(x^n,y^m)}{N_x N_y},
\end{equation}

CoDpy function: 
$$D_k\big(X,Y\big)^2 = \text{ op.Dnm}(X,Y,k),\| f \|_{\mathcal{H}_k}=\text{ op.norm}(X,
f(X),k)$$

```{python,  label =ScoresExecution, fig.cap="Scores and execution time", results = "hide"}
kwargs = {"mp_max_items" :4, "mp_ncols" : 4 }
scenario_generator_.compare_plots(
axis_field_labels = [("Nx","scores"),("Ny","discrepancy_errors")],**kwargs)
```

\newpage

## Clustering method with kernels : sharp discrepancy sequences

CoDpy defines a clustering method as follows:
\begin{equation} \label{cluster}
 \bar{Y} = \arg \inf_{Y \in \RR^{D,N_Y}} D_k(X,Y),
\end{equation}
called \textbf{sharp discrepancy sequences}. This approach is an alternative to more classical clustering algorithms as K-means ones. CoDpy function: 
$$
\text{op.sharp discrepancy}(X,Y,k,...).
$$
```{python,  label =clustering,  fig.cap="Two clustering methods", results = "hide"}
from book_funs2 import list_of_predictors
list_of_predictors,scenario_generator_,df_unsup_results = list_of_predictors()
f_names = ["codpy: sharp discrepancies","Scikit: K-Means"]
multi_plot(list_of_predictors ,graphical_cluster_utilities.plot_clusters, mp_ncols = 4,xlabel = 'x', ylabel = 'y',f_names=f_names)
```


\newpage

### Application of clustering methods I : classification

A natural applications of clustering methods are classification (unsupervised learning). Here is an example of classification of 60 stock market into 10 classes, where the training set $X$ are daily stock price movements $X \in \mathbb{R}^{N_{x} \times D}$ (i.e. the dollar difference between the closing and opening prices for each trading day) from $2010$ to $2015$.


```{python,  results = "hide"}
from book_funs6 import stocks_clustering
scenario_generator_, idx, idx2, stocks_results = stocks_clustering(scenarios_list = [(-1, -1, i,-1) for i in range(5, 6,1)])
```

```{r, label = 29908, warning= FALSE}
idx = cbind(py$idx,py$idx2)
pander::pander(cbind(py$idx,py$idx2), split.cell = 80, split.table = Inf, style = "rmarkdown", caption = "Stock's clustering", col.names = c("Scikit:k-means","CoDpy: sharp disc."))
```

### Application of clustering methods II : computations enhancement

Another quite interesting application to clustering methods are computations enhancement : recall the discrepancy error \eqref{error}:

\begin{equation}
\| f(Z) - f_Z \|_{\ell^2} \le ( D_k(X,Y) +D_k(Y,Z) \Big) \| f \|_{\mathcal{H}_k}
\end{equation}

$\mapsto$ taking $Y$ as a clustering of $X$ into $N_Y$ classes allows to boost accuracy of kernel predictions and differential oeprators with few computational resources as follows:
\begin{equation} 
f_Z := \mathcal{P}_{k}(X,\bar{Y},Z)f(X), \quad \bar{Y} = \arg \inf_{Y \in \RR^{D,N_Y}} D_k(X,Y).
\end{equation}


\newpage

## Generative methods with kernels - the sampler method

A generative method is a method that take as input discrete samples of a given distribution, and reproduce it, hypothesizing that this distribution is continuous. CoDpy function: 
$$
\text{alg.sampler}(X,N,k,...)
$$
For illustration goals, we apply this algorithm for two bi-modal distributions based on a Gaussian and a Student's distribution namely $\mathcal{N}(0,1)$ and  $t(\nu=5)$. We use here two distinct sets (training set $X$ and test set $Z$) to highlight some convergence properties of the sampler algorithm:

\begin{itemize}\setlength{\itemsep}{0pt}
    \item IID : $X,Z$ are iid samples of $\mathbb{X}$.
    \item SDS : $X,Z$ are sharp discrepancy sequences (SDS) of $\mathbb{X}$.
\end{itemize}
For both sets, the size of the training set is $N_x = 1000$, whereas the size of the test set is $N_z=500$. We plot the results computed by the sampler algorithm in Figure \ref{plotiid} (resp. Figure \ref{plotsds}) for the IID case (resp. SDS case).

```{python, label =plotiid, fig.cap="\\label{plotiid} Density of generated IID distributions",results="hide"}
params = generative_methods()
table1 = params['table1']
params['graphic'](**params)
```

Once generated, we compute various statistic indicators to check the similarities between both distributions (historical and generated)

```{r, results = 'asis',label = '101iid'}
pyresults <- py$table1
knitr::kable(pyresults, caption = "Statistics of IID-generated distributions", escape = FALSE)  %>%
      kable_styling(latex_options = c("repeat_header","HOLD_position"))
```

As can be seen, these algorithms are sensitive to input data. In particular, using clustering methods improves algorithm performances.

\newpage

### Application of generative methods : produce samples of any distribution

We outlight here that synthetic data generation is a general approach, and one can consider any distributions, as for instance the MNIST database, consisting of 60000 hand-written digits, having 28x28=784 pixels resolution. We consider it as a discrete 784-dimensional distribution having 60000 samples, this figure showing the first hundred ones
```{python, label =GeneratedMnist,fig.cap = "\\label{mnist} MNIST - distribution of 60 000 hand-written digits", results = 'hide'}
mnist = kernel_image_generator()
```

The incentive to use this data set is to illustrate how our algorithms scale with dimensionality, as well as linking towards classical learning machine problems.

\footnote{We learnt from a distribution consisting of the first 500 over the 60000 hand-written digits of the MNIST database images for performance purposes.}

```{python, label =plotmnistg,fig.cap = "\\label{plotmnistg} Generated distribution of hand-written digits", results = 'hide'}
imshow(mnist['imfz'], cmap='gray')
```

\newpage

## Conditional Expectation Algorithms

Consider any martingale process $t \mapsto X(t)$, and any positive definite kernel $k$, we define the operator $\Pi$ - using python notations -

$$
  f_{Z | X} = \Pi(X,Z,f(Z)) 
$$

where 

- $X \in \mathbb{R}^{ N_x \times D}$ is any set of points generated by a i.i.d sample of $X(t^1)$ where $t^1$ is any time.

- $Z \in \mathbb{R}^{ N_z \times D}$ is any set of points, generated by a i.i.d sample of $X(t^2)$ at any time $t^2>t^1$.

- $f(Z) \in \mathbb{R}^{ N_z \times D_f}$ is any, optional, function. 

The output is a matrix $f_{Z | X}$, representing the conditional expectation 
\begin{equation}
f_{Z | X} \sim \mathbb{E}^{X(t^2)}( f( \cdot ) | X(t^1)) \in \mathbb{R}^{ N_x \times D_f} =:^{not.} f( Z | X). (\#eq:CE)
\end{equation}

- if $f(Z)$ is let empty, the output $f_{Z | X} \in \mathbb{R}^{ N_z \times N_x}$ is a matrix, representing a convergent approximation of the stochastic matrix $\mathbb{E}^{X(t^1)}(Z | X)$.

- if $f(Z) \in \mathbb{R}^{ N_z \times D_f}$ is not empty, $f_{Z | X} \in \mathbb{R}^{ N_z \times D_f}$ is a stochastic matrix, representing the conditional expectation $f(Z |X) := \mathbb{E}^{X(t^1)}( f(Z) | X)$.

$$
  \text{alg.Pi}(X,Z,f(Z)) 
$$
Note : this algorithm is an efficient alternative to \textbf{Sinkhorn} algorithm.

\newpage

# A toy risk framework 
## Application settings - Input data

### Retrieve market data

Let us download real market data, retrieved from January 1, 2016 to December 31, 2021, for three assets: Google, Apple and Amazon. These data are plot Figure \ref{plot1}.

```{python, label =charts,fig.cap = "\\label{plot1} charts for Apple Amazon Google", results = 'hide'}
params = retrieve_market_data()
params['graphic'](**params)
table1 = pd.DataFrame().append({'begin date': params['begin_date'], 'end date': params['end_date'],'pricing date': params['today_date'],'symbols': params['symbols']}, ignore_index=True)
```
To produce this figure, we use the following global settings:

```{r, results = 'asis',label = 101}
pyresults <- py$table1
knitr::kable(pyresults, caption = "Global settings", escape = FALSE)  %>%
      kable_styling(latex_options = c("repeat_header","HOLD_position"))
```
\newpage

### Set a portfolio of instruments

We define a payoff function as $P(t,x) \mapsto P(t,x) \in \RR^{D_P}$, with $D_P$ corresponding to the number of instrument. We consider here a single instrument $D_P=1$, the instrument being a basket option written on our underlyings. We represent this payoff in a two-dimensional figure with axis basket values in Figure \ref{plot2}.


```{python, label =payoff,fig.cap = "\\label{plot2} A payoff of an basket option", results = 'hide'}
params = get_instrument_param()
params['graphic'](**params)
```

\newpage


### Set a pricing engine

We define a pricing function as a payoff, that is a vector-valued function $(t,x) \mapsto P(t,x) \in \RR^{D_P}$. We represent this pricing function in a two-dimensional figure \ref{plot3} with axis basket values.

The pricing function here is selected as a simple Black and Scholes formula, hence hypothesizing that the basket values are log normal \footnote{this choice is made for performance purposes here, but any pricing function can be plugged in.}

```{python, label =pricer,fig.cap = "\\label{plot3} Pricing as a function of time", results = 'hide'}
params = get_pricer_param()
params['graphic'](**params)
```

\newpage

## Synthetic market data generation

### Fit a model to input data

A model can be described by a stochastic differential equation. For instance consider a log-normal process, described by
$X_t = \mu X_t + X_t \sigma dB_t$, $B_t$ being the standard Brownian motion, having solution $X_t = X_s \exp( (t-s) (\mu - \sigma^2 /2) + \sqrt{t-s} \sigma \mathcal{N}(0,1))$, $\mathcal{N}(0,1)$ being the normal law. Fitting this model to historical data plot at figure \ref{plot1} would consist in fitting the parameters $\mu,\sigma$ to the historical data.

```{python, label =LogReturn,fig.cap = "\\label{plot4} Log return distribution of historical market data", results = 'hide'}
params = get_model_param()
params['graphic'](**params)
```

Synthetic data generation introduces somehow a new paradigm, where known processes (as the standard Brownian $B_t$), are replaced by unknown random variable to fit. So instead of \eqref{SDE}, consider the following problem : define a process, having form
\begin{equation}\label{PLR}
  X_t = X_s \exp( (t-s) \mu + \sqrt{t-s} \mathbb{X}),
\end{equation}
where $\mathbb{X}$ is an unknown random variable to fit to historical data. To that aim, consider the log transformation
\begin{equation}\label{LOG}
  \mathbb{X} = \frac{\ln(X_t) - \ln(X_s) - (t-s) \mu}{\sqrt{t-s}},
\end{equation}
in order to separate the random variable $\mathbb{X}$ to fit to historical data. Figure \ref{plot4} plots the ditribution retrieved from our historical data after the transformation \eqref{PLR}.

\newpage

### Generate the distribution

We then provide a function, producing a continuous sampling function from any discrete input distribution. Figure \ref{plot5} is a result of a resample of the historical distribution

```{python, label =GeneratedLogReturn,fig.cap = "\\label{plot5} Log return distribution of generated market data", results = 'hide'}
params = get_generated_param()
params['graphic'](**params)
stats = stats_df(params['transform_h'], params['transform_g']).T
```

### Check generated distribution

In the table \ref{tab:102}, we compute various statistical indicators, as the fourth moments and Kolmogorov-Smirnov tests, to challenge our generated data against the original one.

```{r, results = 'asis',label = 102}
pyresults <- py$stats
knitr::kable(pyresults, caption = "Stats for historical (generated) data ", escape = FALSE)  %>%
      kable_styling(latex_options = c("repeat_header","HOLD_position"))
```

\newpage

### Build and check generated paths

Ten examples of re-generated paths in figure \ref{plot6}. These paths can be used for Monte-Carlo sampling, and we can also build PDE (Partial Differential Equations) pricers, whatever the dimensions are.

```{python, label =GeneratedPaths,fig.cap = "\\label{plot6} Ten examples of generated paths", results = 'hide'}
params = generated_paths()
params['graphic'](**params)
```

\newpage

## Predictive pricing methods


### Training set - VaR scenarios

According to \eqref{error}, the interpolation error committed by the projection operator \(P_k\) \eqref{projection}, defined on a set \(X\), is driven at any point \(z\) by the quantity \(D_k(z,X)\). We plot at Figure \ref{plot10} the isocontours of this error function for two distinct sets (red dots).

\begin{itemize}
\item (right) $X$ is generated as VaR scenarios for three dates $t^0-1,t^0,t^0+1$, with 10 days horizon.
\item (left) $X$ is the historical data set.
\end{itemize}

The test set is generated as VaR scenarios with 5 days horizon (blue dots).

```{python, label =TrainingAndTestSet, results = "hide", fig.cap="\\label{plot10} Training and test set", fig.height=5, fig.width=15 }
params = get_var_data()
params['graphic'](**params)
```

The blue dots in Figure \ref{plot10} is the test set $Z$, and corresponds to simulated, intraday, market values. This figure motivates the VaR-type scenario dataset on the left-hand side to minimize the interpolation error. Note that using the historical data set,  might be of interest, if only historical data are available.

Notice finally that there are three sets of red points at Figure \ref{plot10}-(a), as we considered VaR scenarios at three different times $t^0-1,t^0,t^0+1$, because we are interested in approximating time derivatives for risk management, as the theta $\partial_t P$.

\newpage

### Predict prices

We plot the results of two methods to extrapolate the pricer function on the test set $Z$ (codpy = kernel prediction, taylor = Taylor second order approximation) in Figure \ref{plot11}.We also plot the reference price (exact = reference price).

```{python, label =PricesOutput, results = "hide",fig.cap="\\label{plot11} Prices output" }
params = predict_prices(params)
params['graphic'](**params)
```

\newpage

### Predict greeks

We can also compute greeks, using the operator $(\nabla P)_Z$ defined at \eqref{grad}. Here too, we plot the results of two methods to extrapolate the gradient of the pricer function on the test set $Z$ (codpy = kernel prediction, taylor = Taylor second order approximation) in Figure \ref{plot12}. We also plot the reference greeks (exact = reference greeks). This figure should thus produce $(\nabla P)_Z=\big((\partial_t P)_Z, (\partial_{x_0} P)_Z, \ldots, (\partial_{x_D} P)_Z\big)$, that are $D+1$ plots.


```{python, label =PredictGreeks, results = "hide",fig.cap="\\label{plot12} Greeks output" }
params = predict_greeks()
params['graphic'](nabla=codpy_nabla,**params)
```

Note that deltas present spurious oscillations. However, using some engineering, we can smooth them out

```{python, label =PredictCorrectGreeks, results = "hide",fig.cap="\\label{plot13} Greeks output after correction" }
params['graphic'](nabla=codpy_nabla_corrected,**params)
```


\newpage

# Conclusions

## What are the main points highlighted by this presentation

\begin{itemize}
\item Synthetic data generation is a general, very handy tool, allowing to model and resample not only any given random variables, but also conditionally to other variables. For instance, we use our algorithms to generate synthetic risk measures conditioned to customers data.

\item In this presentation, we start exploring a quite interesting application to risk modelling : we revisited an existing diffusion model, based on a simple Brownian motions, and propose a general method to calibrate it. Doing so, we hope to model risk sources more accurately.

\item Predictive methods allow to approximate computationally expensive risk valuation functions by learning them from quite few discrete examples. This allows to build fast, real-time, pricing solutions, even for huge portfolios.

\item However, one must be very careful as predictive methods, in the context of synthetic data generation, can be quite challenging, particularly while computing derivatives (greeks). We provide solutions, as clustering methods, to enhance the accuracy of such indicators.


\end{itemize}

## Going further


\begin{itemize}

\item As we can resample from historical data, calibrating  these data to quite general model, we can generate our own Monte-Carlo pricers built on top of these models.


\item In the same vein, we can also build high dimensional PDE pricers using these models, a technology similar to Cox trees, but working whatever the number of risk sources are.

\item PDE pricers avoid the "Monte-Carlo of Monte-Carlo" trap for risk-management systems. They allow to estimate risk measures as EEPE, or CVA, in very efficient manner.

\item This presentation can be seen as a toy-prototype of a risk management system based on these ideas.

\end{itemize}


\begin{thebibliography}{00}

\bibitem{GBRS}
{\sc A. Gretton, K.M. Borgwardt, M. Rasch, B. Sch\"{o}lkopf, and A.J. Smola,}
% {\sc Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte and Sch\"{o}lkopf, Bernhard and Smola, Alexander J.}
A kernel method for the two sample problems,
Proc. 19th Int. Conf. on Neural Information Processing Systems, 2006, pp.~513--520.

\bibitem{BTA}
{\sc A. Berlinet and C. Thomas-Agnan,}
{\it Reproducing kernel Hilbert spaces in probability and statistics,}
Springer Science, Business Media, LLC, 2004.

\bibitem{B2001} Bernhard Sch\"{o}lkopf, Ralf Herbrich, and Alexander J. Smola. A generalized representer theorem. In Computational learning theory, pages 416?426. Springer, 2001.
\bibitem{LMM}
{\sc LeFloch, Philippe G. and Mercier, Jean-Marc and Miryusupov, Shohruh}
CodPy: A Python Library for Machine Learning, Mathematical Finance, and Statistics,  (April 6, 2022). Available at SSRN: https://ssrn.com/abstract=4077158 or http://dx.doi.org/10.2139/ssrn.4077158. CoDpy is available at
\url{https://pypi.org/project/codpy/}


\bibitem{PLF-JMM-estimate}
{\sc P.G. LeFloch and J.-M. Mercier,}
Mesh-free error integration in arbitrary dimensions: a numerical study of discrepancy functions,
{\it Comput. Methods Appl. Mech. Engrg.} 369 (2020), 113245.

\bibitem{PLF-JMM-Wilmott}
{\sc P.G. LeFloch and J.-M. Mercier,}
The transport‐based mesh‐free method: a short review,
{\it Wilmott} vol. 2020, iss. 109, p. 52–57, 2020.


\bibitem{EFO}
{\sc Eckerli, Florian and Osterrieder, Joerg,}
Generative Adversarial Networks in finance: an overview,
{\it Comput. Methods Appl. Mech. Engrg.} http://dx.doi.org/10.48550/ARXIV.2106.06364 (2021).

\end{thebibliography}
