% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{graphics}
\usepackage{amsmath, amsfonts, amsthm, amssymb, amscd,a4wide}
\usepackage{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{epstopdf}
\usepackage{subcaption}
\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]%
{\raggedright\footnotesize #1}}

%\usepackage[linesnumbered,ruled]{algorithm2e}

%%

% ces lignes permet de ne pas mettre en couleurs les liens hypertextes dans le fichier PDF
% cependant il est toujours possible de cliquer dessus
%

\usepackage{xcolor} %package pour les couleurs
\usepackage{tikz} % package principal TikZ
\usetikzlibrary{arrows} %librairieoptionnelle PGF
\usepackage{adjustbox}

\usepackage{slashed}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{linktoc = all}                % hyperref settings
%\hypersetup{pdfborderstyle={/S/U/W 0.5}}  % hyperref settings
\hypersetup{hidelinks}
\hypersetup{bookmarksnumbered}
\pdfstringdefDisableCommands{%
  \def\({}%
  \def\){}%
  \def\\{}%
  \def\infty{\042\036}%
  \def\Tr{Tr }%
}
%%%%%%%%% 
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{theoremdefinition}[definition]{Theorem and Definition}
\newtheorem{remark}[definition]{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{example}[definition]{Example}
\newtheorem{HP}{Highlighted point}
% 
\numberwithin{equation}{section}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand \be   {\begin{equation}}
\newcommand \bel {\begin{equation}\label}
\newcommand \ee   {\end{equation}}
\newcommand \dist {{\mbox{\em dist }}}
\newcommand \sgn {{\text{sgn }}}
\newcommand \meas {{\text{meas }}}
\newcommand \supp {{\text{supp }}}
\newcommand \Id   {{\text{Id}}}
\newcommand \smin {s^{\text{min}}}
\newcommand \smax {s^{\text{max}}}
\newcommand \lmin {{\lam^{\text{min}}}}
\newcommand \lmax {{\lam^{\text{max}}}}
\newcommand \RR    {\mathbb{R}}
\newcommand \NN    {\mathbb{N}}
\newcommand \ZZ    {\mathbb{Z}}
\newcommand \QQ    {\mathbb{Q}}
\newcommand \PP    {\mathbb{P}}
\newcommand \EE    {\mathbb{E}}
\newcommand \Rp    {\mathbb{R}^\plus }
\newcommand \RRR    {\mathbf{R}}
\newcommand \SSS    {\mathbf{S}}
\newcommand \Scal    {\mathcal{S}}
\newcommand \Pcal    {\mathcal{P}}
\newcommand \Tbar {\overline T}
\newcommand \Acal {\mathcal A}
\newcommand \Bcal {\mathcal B}
\newcommand \Ccal    {\mathcal{C}}
\newcommand \Mcal    {\mathcal{M}}
\newcommand \Lcal    {\mathcal{L}}
\newcommand \Jcal    {\mathcal{J}}
\newcommand \Kcal    {\mathcal{K}}
\newcommand \Wbf {\mathbf W}
\newcommand \Hcal    {\mathcal{H}}
\newcommand \Tcal    {\mathcal{T}}
\newcommand \lam   {\lambda}
\newcommand \sig   {\sigma}
\newcommand \gam   {\gamma}
\newcommand \ubar   {\overline u}
\newcommand \HH    {\mathcal{H}}
\newcommand \CC    {\mathcal{C}}
\newcommand \Ncal    {\mathcal{N}}
\newcommand \DDD    {\mathcal{D}}
\newcommand \RN    {{\RR^N}}
\newcommand \eps   {\epsilon}
\newcommand \Lam   {\Lambda}
\newcommand \BB    {{\mathcal B}}
\newcommand \WW    {{\mathcal W}}
\newcommand \MM    {{M}}
\newcommand \AAA    {{\mathcal A}}
\newcommand \JJ    {{\mathcal J}}
\newcommand \II    {{\mathcal I}}
\newcommand \LLL    {{\mathbf L}}
\newcommand \VVV    {{\mathbf V}}
\newcommand \QQQ    {{\mathbf Q}}
\newcommand \Rd    {{\mathbb{R}^d}}
\newcommand \CCD    {{\mathbb{C}^D}}
%
\newcommand \del   {\partial}
\newcommand \blam  {{\underline\lambda}}
\newcommand \lamb  {{\overline\lambda}}
\newcommand \Bzero    {{\mathcal{B}_{\delta_0}}}
\newcommand \Bone    {{\mathcal{B}_{\delta_1}}}
\newcommand \Btwo    {{\mathcal{B}_{\delta_2}}}
\newcommand \la         \langle
\newcommand \ra     \rangle
\newcommand \ab     {\overline a}
\newcommand \mmm  {p}

\newcommand \Ybf {\mathbf Y} 
\newcommand \Sbf {\mathbf S} 
\newcommand \hbf {\mathbf h} 

\newcommand \Sbar {\overline S}
\newcommand \Aund {\underline{\Acal}}
\newcommand \Aove {\overline{\Acal}}

\newcommand \plus {+}

\newcommand \RD {{\mathbb R}^D}


\usepackage{mathrsfs}

%*************************************************************************************
\usepackage{authblk}


\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}



 
 
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black ]
\tikzstyle{io} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black] 
%\tikzstyle{io} = [trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, text - white, draw=black, fill=black!30] 
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{bbox} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, text = white, draw=black, fill=black]
\tikzstyle{pp} = [rectangle, minimum width=3cm, minimum height=0.5cm, text centered, draw=black] 
\tikzstyle{pp1} = [rectangle, draw=black!50, thick, minimum width=0.5cm, minimum height = 0.5cm]
\tikzstyle{crl} = [circle, draw=black!50, thick, minimum size = 1.5cm]
\tikzstyle{crl1} = [circle, draw=black!50, thick, minimum size = 0.7cm]
\tikzstyle{line} = [draw, -latex']
\newsavebox{\tempbox}

\tikzstyle{block} = [draw, rectangle, 
    minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=blue!20, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Generative Models and Time Series Predictions for Financial Applications with Kernels Methods},
  pdfauthor={Jean-Marc Mercier \^{}\textbackslash ddag },
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Generative Models and Time Series Predictions for Financial
Applications with Kernels Methods}
\author{Jean-Marc Mercier \(^\ddag\)
\footnote{MPG-Partners, 136 Boulevard Haussmann, 75008 Paris, France. jean-marc.mercier@mpg-partners.com}}
\date{09 11 2022}

\begin{document}
\maketitle
\begin{abstract}
This presentation deals with generative and predictive models based on
kernel methods. After introducing our kernel library, We illustrate
these models with a toy-example of risk framework based on time series
forecasting, that could be used for real-time P\&L explanation of large
derivative portfolios, or other risk measures as Expected Positive
Exposure or Credit Valuation Risk.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In this jupyter notebook presentation, we would like to introduce our
internal AI library, highlighting it with two interesting applications
of machine learning to finance, namely:

\begin{itemize}
\item Synthetic data generation. For finance applications, there exists numerous applications, among them are time series forecast of risk sources that we illustrate in this presentation. The most-known technologies for producing synthetic datas are neural networks based, as for instance GAN, WGAN, CLGAN, that are at heart of synthetic images or videos production.
\item Predictives methods. Here too, applications for finance are numerous, and we illustrate here an application to real-time P\&L computations. The most-known technologies for predictive methods are for instance neural networks, decision trees, etc...
\end{itemize}

For this presentation, we used our open source library, codpy
\footnote{The codpy user manual is accessible 
\href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4077158}{clicking here}. For installation, follow the guidelines \href{https://pypi.org/project/codpy/}{clicking here}.}.
Codpy is a kernel based technology (RHKS - Reproducing Kernel Hilbert
Space theory), that we have developped and are using for the internal
needs at MPG-Partners. Codpy is an alternative library to other AI
libraries (pytorch, theano, tensorflow, etc\ldots) that has some nice
properties for finance applications:

\begin{itemize}
\item It is an explainable method. All produced results come with computable error estimates allowing to qualify them. Error estimates allow moreover to link naturally to Optimal Transport Theory based tools, used thoroughly by this library.  
\item It is a performing and accurate library, particularly while dealing with sparse input data (small training set).
\item It can compute efficiently a wide zoo of differential operators. Indeed, this library was thought primarily to solve mathematical physics based problems, and is used to approximate some dynamical systems described by a PDE (Partial Differential Equations) model.
\end{itemize}

\newpage

This presentation is structured as follows :

\tableofcontents

\newpage

\hypertarget{a-quick-tour-to-artificial-intelligence---five-codpy-python-methods}{%
\section{A quick tour to Artificial Intelligence - Five CoDpy python
methods}\label{a-quick-tour-to-artificial-intelligence---five-codpy-python-methods}}

\hypertarget{predictive-methods}{%
\subsection{Predictive methods}\label{predictive-methods}}

\hypertarget{training-sets-and-test-sets}{%
\subsubsection{Training sets and test
sets}\label{training-sets-and-test-sets}}

Artificial Intelligence (AI) is mainly based over predictions, that are
nothing else but extrapolation methods.

\textbf{AI Vocabulary:}. \(X,f(X)\) is the \textbf{training set}. \(Z\)
is the \textbf{test set}, \(f_z\) is the \textbf{prediction set},
\(f(Z)\) is the reference (ground truth) value. \(Y\) is the
\textbf{parameter set}, that are internal parameters to a prediction
algorithm.

A simple example is given below : a collection of 100 one dimensional
points \(X\) lying on the segment {[}-1,1{]}, together with their values
given by a sinusoidal function \(f(X)\). The test set are points on the
segment {[}-1.5,1.5{]}, allowing to check extrapolations behaviors.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/xfxzfz-1.pdf}
\caption{training set, test set and ground truth values for a one
dimensional example.}
\end{figure}

Another example of more elaborate training set : the MNIST database,
composed of \(60,000\) images defining a training set of handwritten
digits. Each image is a vector having dimensions \(784\) (a
\(28 \times 28\) grayscale image flattened in row-order), with their
labels, \(0–9\). The test set is composed of \(10,000\) images.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/mnist-3.pdf}
\caption{\label{mnist} MNIST - distribution of 60 000 hand-written
digits}
\end{figure}

\newpage

\hypertarget{predictive-methods-1}{%
\subsubsection{Predictive methods}\label{predictive-methods-1}}

There exists a lot of predictive machines or algorithms. To unify them,
a predictor, denoted by \(\mathcal{P}_m\), can be described as an
extrapolation or interpolation operator, that defines a prediction as
follows: \begin{equation}\label{eq:Pm}
f_Z = \mathcal{P}_{m}(X,Y=[],Z=X,f(X)).
\end{equation} \(f_Z\), the prediction, has to be compared to \(f(Z)\),
the ground truth values.

Codpy provides facilities to compare methods, as illustrated in these
figures, retrieved from our user manual for the one-dimensional problem.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/list_of_list_results-5.pdf}
\caption{Example of predictions : Periodic kernel:CodPy, the RBF kernel:
SciPy, SVR: Scikit, Neural Network: TensorFlow, Decision tree: Scikit,
Adaboost: Scikit, XGBoost, Random Forest: Scikit}
\end{figure}

\newpage

\hypertarget{benchmark-of-predictive-methods}{%
\subsubsection{Benchmark of predictive
methods}\label{benchmark-of-predictive-methods}}

Once a prediction \(f_Z\) is made, one want to compare to the ground
truth values \(f(Z)\), using some metrics, here a confusion matrix for
the MNIST problem for codpy prediction method

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/Confusion-7.pdf}
\caption{Confusion matrix for codpy: Classifier}
\end{figure}

For classification problems as MNIST, scores is a normalized metric (the
higher the better) : \[
  0 \le \frac{\# \{f_Z == f(Z)\}}{N_Z} \le 1
\]

Below are scores for the MNIST for several projection methods for
different settings, corresponding to varying training set sizes.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/Scores-9.pdf}
\caption{Scores and execution time}
\end{figure}

\newpage

\hypertarget{kernel-projection-operator}{%
\subsubsection{Kernel projection
operator}\label{kernel-projection-operator}}

Kernel methods define a quite simple prediction operator, that we
present now briefly. Let \(X\) (resp. \(Y,Z\)) be a set of \(N_x\)
(resp. \(N_y,N_Z\)) \textbf{distinct} points in dimension \(D\), \(f\)
any continuous, vector valued, function. Notations
\begin{equation}\label{X}
X :=\{x^n_d\}_{n,d=1}^{N_x,D} \in \RR^{N_x,D}, \quad f(X) \in \RR^{N_x,D_f}
\end{equation} Let \(k\) be a kernel, that is a symmetric and positive
definite (see \cite{BTA} for a definition) scalar function
\(k: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\). Consider
\(K(X,Y)\) the Gram matrix,
i.e.~\(K(X,Y):=(k(x^n,y^m))_{n,m = 1}^{N_x,N_y}\). We define a
\textbf{prediction} as \begin{equation} \label{projection}
f_Z := \mathcal{P}_{k}(X,Y,Z)f(X), \quad \mathcal{P}_{k}(X,Y,Z) := K(Y,Z) K(X,Y)^{-1}.
\end{equation} The inverse is computed using a least-squares approach,
as follows, \(K(X,Y)^{-1} = ( K(Y,X)K(X,Y)+ \epsilon)^{-1}K(Y,X)\),
where \(\epsilon\) is a (optional) regularization term, as is the
Tychonov regularization method, or other ones. A classical choice for
the parameter set are \(Y=X\) (extrapolation), or \(Y \subset X\)
(interpolation - Nystrom method). Starting from the formula
\eqref{projection}, we can define all kind of differential operators, as
for instance the gradient \begin{equation} \label{grad}
\nabla f_Z = (\nabla_Z K)(Y,Z) K(X,Y)^{-1} f(X).
\end{equation} CoDpy function: \[
f_z = \text{ op.projection}(X,Y,Z,f(X), k,...),\nabla f_Z = \text{op.nabla}(X,Y,Z,f(X), k,...)
\].

\newpage

\hypertarget{error-estimates-with-kernels-dicrepancies-and-norms}{%
\subsection{Error estimates with kernels : Dicrepancies and
norms}\label{error-estimates-with-kernels-dicrepancies-and-norms}}

The projection operator \eqref{projection} benefits from the following
error estimate (see \cite{PLF-JMM-estimate}), that are confidence levels
\begin{equation} \label{error}
\| f(Z) - f_Z \|_{\ell^2} \le D_k(X,Y,Z) \| f \|_{\mathcal{H}_k},
\end{equation} where \(D_k(X,Y,Z) := D_k(X,Y)+D_k(Y,Z)\), and where the
discrepancy (called also MMD - Maximum Mean Discrepancy) between two
discrete probability measures \(X\) and \(Y\), induced by a kernel \(k\)
is \begin{equation}\label{Dk}
    D_k\big(X,Y\big)^2 := \frac{ \sum_{n,m=1}^{N_x}  k(x^n,x^m)}{N_x^2} + \frac{\sum_{n,m}^{N_y} k(y^n,y^m)}{N_y^2} - \frac{2 \sum_{n,m=1}^{N_x,N_y} k(x^n,y^m)}{N_x N_y},
\end{equation}

CoDpy function:
\[D_k\big(X,Y\big)^2 = \text{ op.Dnm}(X,Y,k),\| f \|_{\mathcal{H}_k}=\text{ op.norm}(X,
f(X),k)\]

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/ScoresExecution-11.pdf}
\caption{Scores and execution time}
\end{figure}

\newpage

\hypertarget{clustering-method-with-kernels-sharp-discrepancy-sequences}{%
\subsection{Clustering method with kernels : sharp discrepancy
sequences}\label{clustering-method-with-kernels-sharp-discrepancy-sequences}}

CoDpy defines a clustering method as follows:
\begin{equation} \label{cluster}
 \bar{Y} = \arg \inf_{Y \in \RR^{D,N_Y}} D_k(X,Y),
\end{equation} called \textbf{sharp discrepancy sequences}. This
approach is an alternative to more classical clustering algorithms as
K-means ones. CoDpy function: \[
\text{op.sharp discrepancy}(X,Y,k,...).
\] \includegraphics{./CodPyTempFigs/clustering-13.pdf}

\newpage

\hypertarget{application-of-clustering-methods-i-classification}{%
\subsubsection{Application of clustering methods I :
classification}\label{application-of-clustering-methods-i-classification}}

A natural applications of clustering methods are classification
(unsupervised learning). Here is an example of classification of 60
stock market into 10 classes, where the training set \(X\) are daily
stock price movements \(X \in \mathbb{R}^{N_{x} \times D}\) (i.e.~the
dollar difference between the closing and opening prices for each
trading day) from \(2010\) to \(2015\).

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0150}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5787}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4064}}@{}}
\caption{Stock's clustering}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Scikit:k-means
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
CoDpy: sharp disc.
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
~
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Scikit:k-means
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
CoDpy: sharp disc.
\end{minipage} \\
\midrule()
\endhead
\textbf{0} & Amazon, Canon, Cisco, Ford, Google/Alphabet, Honda, HP,
Mitsubishi, SAP, Sony, Sanofi-Aventis, Toyota, Xerox & AIG, American
express, Bank of America, Goldman Sachs, JPMorgan Chase, Wells Fargo \\
\textbf{1} & American express, Boeing, British American Tobacco, DuPont
de Nemours, Dell, General Electrics, GlaxoSmithKline, Home Depot, IBM,
Intel, Lookheed Martin, MasterCard, McDonalds, 3M, Microsoft, Northrop
Grumman, Novartis, Symantec, Total, Taiwan Semiconductor Manufacturing,
Texas instruments, Unilever, Yahoo & Colgate-Palmolive, Johnson \&
Johnson, Kimberly-Clark, Coca Cola, Pepsi, Procter Gamble, Wal-Mart \\
\textbf{2} & AIG, Bank of America, Goldman Sachs, JPMorgan Chase, Wells
Fargo & Apple, Caterpillar, ConocoPhillips, Chevron, DuPont de Nemours,
General Electrics, IBM, 3M, Pfizer, Schlumberger, Valero Energy,
Exxon \\
\textbf{3} & Colgate-Palmolive, Johnson \& Johnson, Kimberly-Clark, Coca
Cola, Pepsi, Pfizer, Procter Gamble, Philip Morris, Walgreen, Wal-Mart &
Amazon, Boeing, Canon, Cisco, Dell, Ford, Google/Alphabet, Honda, HP,
Intel, Lookheed Martin, Mitsubishi, Navistar, Northrop Grumman, Sony,
Toyota, Taiwan Semiconductor Manufacturing, Texas instruments, Xerox,
Yahoo \\
\textbf{4} & Apple, Caterpillar, ConocoPhillips, Chevron, Navistar,
Royal Dutch Shell, Schlumberger, Valero Energy, Exxon & British American
Tobacco, GlaxoSmithKline, Home Depot, MasterCard, McDonalds, Microsoft,
Novartis, Philip Morris, Royal Dutch Shell, SAP, Sanofi-Aventis,
Symantec, Total, Unilever, Walgreen \\
\bottomrule()
\end{longtable}

\hypertarget{application-of-clustering-methods-ii-computations-enhancement}{%
\subsubsection{Application of clustering methods II : computations
enhancement}\label{application-of-clustering-methods-ii-computations-enhancement}}

Another quite interesting application to clustering methods are
computations enhancement : recall the discrepancy error \eqref{error}:

\begin{equation}
\| f(Z) - f_Z \|_{\ell^2} \le ( D_k(X,Y) +D_k(Y,Z) \Big) \| f \|_{\mathcal{H}_k}
\end{equation}

\(\mapsto\) taking \(Y\) as a clustering of \(X\) into \(N_Y\) classes
allows to boost accuracy of kernel predictions and differential
oeprators with few computational resources as follows: \begin{equation} 
f_Z := \mathcal{P}_{k}(X,\bar{Y},Z)f(X), \quad \bar{Y} = \arg \inf_{Y \in \RR^{D,N_Y}} D_k(X,Y).
\end{equation}

\newpage

\hypertarget{generative-methods-with-kernels---the-sampler-method}{%
\subsection{Generative methods with kernels - the sampler
method}\label{generative-methods-with-kernels---the-sampler-method}}

A generative method is a method that take as input discrete samples of a
given distribution, and reproduce it, hypothesizing that this
distribution is continuous. CoDpy function: \[
\text{alg.sampler}(X,N,k,...)
\] For illustration goals, we apply this algorithm for two bi-modal
distributions based on a Gaussian and a Student's distribution namely
\(\mathcal{N}(0,1)\) and \(t(\nu=5)\). We use here two distinct sets
(training set \(X\) and test set \(Z\)) to highlight some convergence
properties of the sampler algorithm:

\begin{itemize}\setlength{\itemsep}{0pt}
    \item IID : $X,Z$ are iid samples of $\mathbb{X}$.
    \item SDS : $X,Z$ are sharp discrepancy sequences (SDS) of $\mathbb{X}$.
\end{itemize}

For both sets, the size of the training set is \(N_x = 1000\), whereas
the size of the test set is \(N_z=500\). We plot the results computed by
the sampler algorithm in Figure \ref{plotiid} (resp. Figure
\ref{plotsds}) for the IID case (resp. SDS case).

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/plotiid-1.pdf}
\caption{\label{plotiid} Density of generated IID distributions}
\end{figure}

Once generated, we compute various statistic indicators to check the
similarities between both distributions (historical and generated)

\begin{table}[H]

\caption{\label{tab:101iid}Statistics of IID-generated distributions}
\centering
\begin{tabular}[t]{l|l|l|l|l|l}
\hline
  & Mean & Variance & Skewness & Kurtosis & KS test\\
\hline
IID sequences & 0.047(0.74) & 0.012(-0.068) & 26(26) & -1.8(-1.9) & 7.2e-12(0.05)\\
\hline
Sharp Discr. sequences & 0.047(0.2) & 0.012(-0.06) & 26(26) & -1.8(-1.9) & 0.13(0.05)\\
\hline
\end{tabular}
\end{table}

As can be seen, these algorithms are sensitive to input data. In
particular, using clustering methods improves algorithm performances.

\newpage

\hypertarget{application-of-generative-methods-produce-samples-of-any-distribution}{%
\subsubsection{Application of generative methods : produce samples of
any
distribution}\label{application-of-generative-methods-produce-samples-of-any-distribution}}

We outlight here that synthetic data generation is a general approach,
and one can consider any distributions, as for instance the MNIST
database, consisting of 60000 hand-written digits, having 28x28=784
pixels resolution. We consider it as a discrete 784-dimensional
distribution having 60000 samples, this figure showing the first hundred
ones

The incentive to use this data set is to illustrate how our algorithms
scale with dimensionality, as well as linking towards classical learning
machine problems.

\footnote{We learnt from a distribution consisting of the first 500 over the 60000 hand-written digits of the MNIST database images for performance purposes.}

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/plotmnistg-1.pdf}
\caption{\label{plotmnistg} Generated distribution of hand-written
digits}
\end{figure}

\newpage

\hypertarget{conditional-expectation-algorithms}{%
\subsection{Conditional Expectation
Algorithms}\label{conditional-expectation-algorithms}}

Consider any martingale process \(t \mapsto X(t)\), and any positive
definite kernel \(k\), we define the operator \(\Pi\) - using python
notations -

\[
  f_{Z | X} = \Pi(X,Z,f(Z)) 
\]

where

\begin{itemize}
\item
  \(X \in \mathbb{R}^{ N_x \times D}\) is any set of points generated by
  a i.i.d sample of \(X(t^1)\) where \(t^1\) is any time.
\item
  \(Z \in \mathbb{R}^{ N_z \times D}\) is any set of points, generated
  by a i.i.d sample of \(X(t^2)\) at any time \(t^2>t^1\).
\item
  \(f(Z) \in \mathbb{R}^{ N_z \times D_f}\) is any, optional, function.
\end{itemize}

The output is a matrix \(f_{Z | X}\), representing the conditional
expectation \begin{equation}
f_{Z | X} \sim \mathbb{E}^{X(t^2)}( f( \cdot ) | X(t^1)) \in \mathbb{R}^{ N_x \times D_f} =:^{not.} f( Z | X). (\#eq:CE)
\end{equation}

\begin{itemize}
\item
  if \(f(Z)\) is let empty, the output
  \(f_{Z | X} \in \mathbb{R}^{ N_z \times N_x}\) is a matrix,
  representing a convergent approximation of the stochastic matrix
  \(\mathbb{E}^{X(t^1)}(Z | X)\).
\item
  if \(f(Z) \in \mathbb{R}^{ N_z \times D_f}\) is not empty,
  \(f_{Z | X} \in \mathbb{R}^{ N_z \times D_f}\) is a stochastic matrix,
  representing the conditional expectation
  \(f(Z |X) := \mathbb{E}^{X(t^1)}( f(Z) | X)\).
\end{itemize}

\[
  \text{alg.Pi}(X,Z,f(Z)) 
\] Note : this algorithm is an efficient alternative to
\textbf{Sinkhorn} algorithm.

\newpage

\hypertarget{a-toy-risk-framework}{%
\section{A toy risk framework}\label{a-toy-risk-framework}}

\hypertarget{application-settings---input-data}{%
\subsection{Application settings - Input
data}\label{application-settings---input-data}}

\hypertarget{retrieve-market-data}{%
\subsubsection{Retrieve market data}\label{retrieve-market-data}}

Let us download real market data, retrieved from January 1, 2016 to
December 31, 2021, for three assets: Google, Apple and Amazon. These
data are plot Figure \ref{plot1}.

\includegraphics{./CodPyTempFigs/charts-3.pdf} To produce this figure,
we use the following global settings:

\begin{table}[H]

\caption{\label{tab:101}Global settings}
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
begin date & end date & pricing date & symbols\\
\hline
01/06/2016 & 01/06/2022 & 01/06/2022 & AAPL , GOOGL, AMZN\\
\hline
\end{tabular}
\end{table}
\newpage

\hypertarget{set-a-portfolio-of-instruments}{%
\subsubsection{Set a portfolio of
instruments}\label{set-a-portfolio-of-instruments}}

We define a payoff function as \(P(t,x) \mapsto P(t,x) \in \RR^{D_P}\),
with \(D_P\) corresponding to the number of instrument. We consider here
a single instrument \(D_P=1\), the instrument being a basket option
written on our underlyings. We represent this payoff in a
two-dimensional figure with axis basket values in Figure \ref{plot2}.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/payoff-1.pdf}
\caption{\label{plot2} A payoff of an basket option}
\end{figure}

\newpage

\hypertarget{set-a-pricing-engine}{%
\subsubsection{Set a pricing engine}\label{set-a-pricing-engine}}

We define a pricing function as a payoff, that is a vector-valued
function \((t,x) \mapsto P(t,x) \in \RR^{D_P}\). We represent this
pricing function in a two-dimensional figure \ref{plot3} with axis
basket values.

The pricing function here is selected as a simple Black and Scholes
formula, hence hypothesizing that the basket values are log normal
\footnote{this choice is made for performance purposes here, but any pricing function can be plugged in.}

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/pricer-3.pdf}
\caption{\label{plot3} Pricing as a function of time}
\end{figure}

\newpage

\hypertarget{synthetic-market-data-generation}{%
\subsection{Synthetic market data
generation}\label{synthetic-market-data-generation}}

\hypertarget{fit-a-model-to-input-data}{%
\subsubsection{Fit a model to input
data}\label{fit-a-model-to-input-data}}

A model can be described by a stochastic differential equation. For
instance consider a log-normal process, described by
\(X_t = \mu X_t + X_t \sigma dB_t\), \(B_t\) being the standard Brownian
motion, having solution
\(X_t = X_s \exp( (t-s) (\mu - \sigma^2 /2) + \sqrt{t-s} \sigma \mathcal{N}(0,1))\),
\(\mathcal{N}(0,1)\) being the normal law. Fitting this model to
historical data plot at figure \ref{plot1} would consist in fitting the
parameters \(\mu,\sigma\) to the historical data.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/LogReturn-5.pdf}
\caption{\label{plot4} Log return distribution of historical market
data}
\end{figure}

Synthetic data generation introduces somehow a new paradigm, where known
processes (as the standard Brownian \(B_t\)), are replaced by unknown
random variable to fit. So instead of \eqref{SDE}, consider the
following problem : define a process, having form
\begin{equation}\label{PLR}
  X_t = X_s \exp( (t-s) \mu + \sqrt{t-s} \mathbb{X}),
\end{equation} where \(\mathbb{X}\) is an unknown random variable to fit
to historical data. To that aim, consider the log transformation
\begin{equation}\label{LOG}
  \mathbb{X} = \frac{\ln(X_t) - \ln(X_s) - (t-s) \mu}{\sqrt{t-s}},
\end{equation} in order to separate the random variable \(\mathbb{X}\)
to fit to historical data. Figure \ref{plot4} plots the ditribution
retrieved from our historical data after the transformation \eqref{PLR}.

\newpage

\hypertarget{generate-the-distribution}{%
\subsubsection{Generate the
distribution}\label{generate-the-distribution}}

We then provide a function, producing a continuous sampling function
from any discrete input distribution. Figure \ref{plot5} is a result of
a resample of the historical distribution

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/GeneratedLogReturn-7.pdf}
\caption{\label{plot5} Log return distribution of generated market data}
\end{figure}

\hypertarget{check-generated-distribution}{%
\subsubsection{Check generated
distribution}\label{check-generated-distribution}}

In the table \ref{tab:102}, we compute various statistical indicators,
as the fourth moments and Kolmogorov-Smirnov tests, to challenge our
generated data against the original one.

\begin{table}[H]

\caption{\label{tab:102}Stats for historical (generated) data }
\centering
\begin{tabular}[t]{l|l|l|l}
\hline
  & AAPL & AMZN & GOOGL\\
\hline
Mean & 0.0013(0.002) & 0.001(0.0018) & 0.00073(0.0012)\\
\hline
Variance & -0.48(-0.26) & -0.14(0.026) & -0.48(-0.26)\\
\hline
Skewness & 0.0003(0.00025) & 0.00031(0.00027) & 0.00025(0.00019)\\
\hline
Kurtosis & 7.4(4.2) & 3.7(3) & 6.5(3.5)\\
\hline
KS test & 0.41(0.05) & 0.36(0.05) & 0.49(0.05)\\
\hline
\end{tabular}
\end{table}

\newpage

\hypertarget{build-and-check-generated-paths}{%
\subsubsection{Build and check generated
paths}\label{build-and-check-generated-paths}}

Ten examples of re-generated paths in figure \ref{plot6}. These paths
can be used for Monte-Carlo sampling, and we can also build PDE (Partial
Differential Equations) pricers, whatever the dimensions are.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/GeneratedPaths-1.pdf}
\caption{\label{plot6} Ten examples of generated paths}
\end{figure}

\newpage

\hypertarget{predictive-pricing-methods}{%
\subsection{Predictive pricing
methods}\label{predictive-pricing-methods}}

\hypertarget{training-set---var-scenarios}{%
\subsubsection{Training set - VaR
scenarios}\label{training-set---var-scenarios}}

According to \eqref{error}, the interpolation error committed by the
projection operator \(P_k\) \eqref{projection}, defined on a set \(X\),
is driven at any point \(z\) by the quantity \(D_k(z,X)\). We plot at
Figure \ref{plot10} the isocontours of this error function for two
distinct sets (red dots).

\begin{itemize}
\item (right) $X$ is generated as VaR scenarios for three dates $t^0-1,t^0,t^0+1$, with 10 days horizon.
\item (left) $X$ is the historical data set.
\end{itemize}

The test set is generated as VaR scenarios with 5 days horizon (blue
dots).

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/TrainingAndTestSet-3.pdf}
\caption{\label{plot10} Training and test set}
\end{figure}

The blue dots in Figure \ref{plot10} is the test set \(Z\), and
corresponds to simulated, intraday, market values. This figure motivates
the VaR-type scenario dataset on the left-hand side to minimize the
interpolation error. Note that using the historical data set, might be
of interest, if only historical data are available.

Notice finally that there are three sets of red points at Figure
\ref{plot10}-(a), as we considered VaR scenarios at three different
times \(t^0-1,t^0,t^0+1\), because we are interested in approximating
time derivatives for risk management, as the theta \(\partial_t P\).

\newpage

\hypertarget{predict-prices}{%
\subsubsection{Predict prices}\label{predict-prices}}

We plot the results of two methods to extrapolate the pricer function on
the test set \(Z\) (codpy = kernel prediction, taylor = Taylor second
order approximation) in Figure \ref{plot11}.We also plot the reference
price (exact = reference price).

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/PricesOutput-5.pdf}
\caption{\label{plot11} Prices output}
\end{figure}

\newpage

\hypertarget{predict-greeks}{%
\subsubsection{Predict greeks}\label{predict-greeks}}

We can also compute greeks, using the operator \((\nabla P)_Z\) defined
at \eqref{grad}. Here too, we plot the results of two methods to
extrapolate the gradient of the pricer function on the test set \(Z\)
(codpy = kernel prediction, taylor = Taylor second order approximation)
in Figure \ref{plot12}. We also plot the reference greeks (exact =
reference greeks). This figure should thus produce
\((\nabla P)_Z=\big((\partial_t P)_Z, (\partial_{x_0} P)_Z, \ldots, (\partial_{x_D} P)_Z\big)\),
that are \(D+1\) plots.

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/PredictGreeks-7.pdf}
\caption{\label{plot12} Greeks output}
\end{figure}

Note that deltas present spurious oscillations. However, using some
engineering, we can smooth them out

\begin{figure}
\centering
\includegraphics{./CodPyTempFigs/PredictCorrectGreeks-9.pdf}
\caption{\label{plot13} Greeks output after correction}
\end{figure}

\newpage

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

\hypertarget{what-are-the-main-points-highlighted-by-this-presentation}{%
\subsection{What are the main points highlighted by this
presentation}\label{what-are-the-main-points-highlighted-by-this-presentation}}

\begin{itemize}
\item Synthetic data generation is a general, very handy tool, allowing to model and resample not only any given random variables, but also conditionally to other variables. For instance, we use our algorithms to generate synthetic risk measures conditioned to customers data.

\item In this presentation, we start exploring a quite interesting application to risk modelling : we revisited an existing diffusion model, based on a simple Brownian motions, and propose a general method to calibrate it. Doing so, we hope to model risk sources more accurately.

\item Predictive methods allow to approximate computationally expensive risk valuation functions by learning them from quite few discrete examples. This allows to build fast, real-time, pricing solutions, even for huge portfolios.

\item However, one must be very careful as predictive methods, in the context of synthetic data generation, can be quite challenging, particularly while computing derivatives (greeks). We provide solutions, as clustering methods, to enhance the accuracy of such indicators.


\end{itemize}

\hypertarget{going-further}{%
\subsection{Going further}\label{going-further}}

\begin{itemize}

\item As we can resample from historical data, calibrating  these data to quite general model, we can generate our own Monte-Carlo pricers built on top of these models.


\item In the same vein, we can also build high dimensional PDE pricers using these models, a technology similar to Cox trees, but working whatever the number of risk sources are.

\item PDE pricers avoid the "Monte-Carlo of Monte-Carlo" trap for risk-management systems. They allow to estimate risk measures as EEPE, or CVA, in very efficient manner.

\item This presentation can be seen as a toy-prototype of a risk management system based on these ideas.

\end{itemize}

\begin{thebibliography}{00}

\bibitem{GBRS}
{\sc A. Gretton, K.M. Borgwardt, M. Rasch, B. Sch\"{o}lkopf, and A.J. Smola,}
% {\sc Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte and Sch\"{o}lkopf, Bernhard and Smola, Alexander J.}
A kernel method for the two sample problems,
Proc. 19th Int. Conf. on Neural Information Processing Systems, 2006, pp.~513--520.

\bibitem{BTA}
{\sc A. Berlinet and C. Thomas-Agnan,}
{\it Reproducing kernel Hilbert spaces in probability and statistics,}
Springer Science, Business Media, LLC, 2004.

\bibitem{B2001} Bernhard Sch\"{o}lkopf, Ralf Herbrich, and Alexander J. Smola. A generalized representer theorem. In Computational learning theory, pages 416?426. Springer, 2001.
\bibitem{LMM}
{\sc LeFloch, Philippe G. and Mercier, Jean-Marc and Miryusupov, Shohruh}
CodPy: A Python Library for Machine Learning, Mathematical Finance, and Statistics,  (April 6, 2022). Available at SSRN: https://ssrn.com/abstract=4077158 or http://dx.doi.org/10.2139/ssrn.4077158. CoDpy is available at
\url{https://pypi.org/project/codpy/}


\bibitem{PLF-JMM-estimate}
{\sc P.G. LeFloch and J.-M. Mercier,}
Mesh-free error integration in arbitrary dimensions: a numerical study of discrepancy functions,
{\it Comput. Methods Appl. Mech. Engrg.} 369 (2020), 113245.

\bibitem{PLF-JMM-Wilmott}
{\sc P.G. LeFloch and J.-M. Mercier,}
The transport‐based mesh‐free method: a short review,
{\it Wilmott} vol. 2020, iss. 109, p. 52–57, 2020.


\bibitem{EFO}
{\sc Eckerli, Florian and Osterrieder, Joerg,}
Generative Adversarial Networks in finance: an overview,
{\it Comput. Methods Appl. Mech. Engrg.} http://dx.doi.org/10.48550/ARXIV.2106.06364 (2021).

\end{thebibliography}

\end{document}
