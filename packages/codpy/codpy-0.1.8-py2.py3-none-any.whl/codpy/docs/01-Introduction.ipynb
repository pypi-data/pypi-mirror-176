{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from preamble import *\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-------- \n",
                "\n",
                "-------- \n",
                "\n",
                "-------- \n",
                "\n",
                "-------- \n",
                "\n",
                "This a preliminary version of a monograph in preparation This version: April 5, 2022.\n",
                "\n",
                "---------\n",
                "\n",
                "# Introduction\n",
                "\n",
                "## Main objective  \n",
                "\n",
                "This monograph presents the algorithms that are implemented in the Python library CodPy ---an acronym that stands for the \"Curse Of Dimensionality in PYthon\". This library provides the user with a support vector machine (SVM) and encompasses a broad set of applications. The proposed algorithms apply to partial differential equations arising, for instance, in mathematical finance and fluid dynamics, as well as to discrete models arising in machine learning and statistics. Our basic numerical strategy relies on a combination of ideas from the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. The authors have developed this library over the past decade, originally for applications in mathematical finance, and it has now reach a stage where it can be applied to problems of interest to engineering and industry.\n",
                "\n",
                "We proceed by presenting first, in several tutorial chapters (Chapters 2 to 4), fundamental notions about kernel-based discretizations as we proposed for the CodPy library, and at this stage we only include elementary examples which illustrate the relevance of these fundamental concepts. In the second part of this monograph (Chapters 5 to 8), we apply this general framework and introduce further (and somewhat more involved) discretization techniques while presenting numerical results and applications. Here, we treat several important problems arising in pattern recognition and mathematical finance. Kernel-engineering technique are discussed and aim at formulating support vector machines in a way that makes it easy to adapt them to many problems of interest in engineering and industry. \n",
                "\n",
                "The methodology applies to the discretization of partial differential operators and leads us to kernel-based building blocks (associated with any given support vector machine) which can be used for the approximation of solutions to partial differential problems, such those arising in fluid dynamics modeling. Importantly, to all of our numerical results we associate quantitative error bounds (or quality tests), which are of crucial importance in applications especially in mathematical finance. \n",
                "\n",
                "We found it convenient to write this monograph by relying on a combination of Python code, R code, and Latex code. We produced a Jupyter notebook in which all of the numerical tests can be repeated and modified by the reader\\footnote{The CodPy library is available for download.}. \n",
                "\n",
                "## Outline of this monograph \n",
                "\n",
                "* In Chapter 2 we provide the reader with a brief overview of techniques of machine learning and we introduce the notation and terminology we will use in this monograph while pointing out some other terminology also used within the machine learning community. Here, we thus discuss the notions relevant for  \n",
                "\n",
                "  * the description of numerical algorithms of machine learning, \n",
                "\n",
                "  * the description of performance indicators (or error estimates) which provide a measure of the relevance of any given learning machine, and \n",
                "\n",
                "  * we briefly list the class of libraries currently available.\n",
                "\n",
                "It is out of the scope of this monograph to cover all of the techniques that are currently available, and we restrict attention to a selection of kernel-based methods of machine learning and specific applications of central interest. By our own presentation of this material, we attempt here to provide a new insight on the subject. \n",
                "\n",
                "Kernel-based projection operators and kernel-based clustering methods presented in Chapter 3 are novel algorithms that have no equivalent formulation in the existing algorithmic literature. We also advocate here the use of the notion of discrepancy error and kernel-based norms which lead us to performance indicators with good efficiency in the applications.  \n",
                "\n",
                "We provide criteria that can be used to evaluate the performance of an algorithm and do not depend on the specific method in use. This leads us to a suitable benchmark of existing methods. Many methods of machine learning have been proposed in the literature, and we advocate the use of the above indicator in order to  systematically benchmark them. \n",
                "  * To any given learning problem, represented by a list of input data,  \n",
                "  * one should pick up several scenarios, several learning machines, and several performance indicators, and then \n",
                "  * systematically run the corresponding tests in order to compare, from the output, the various performance indicators.\n",
                "\n",
                "We consider here several learning machines, and compare with our strategy first with one- or two-dimensional examples, leading to relevant benchmarks for supervised learning and unsupervised learning. We then proceed with the study of problems of direct interest in the application. \n",
                "\n",
                "The following topics are covered in this monograph.  \n",
                "\n",
                "  * Chapter 2:  Brief overview of methods of machine learning\n",
                "\n",
                "  * Chapter 3: Kernel methods for machine learning\n",
                "\n",
                "  * Chapter 4: Kernel methods for optimal transport\n",
                "\n",
                "  * Chapter 5: Application to supervised machine learning\n",
                "\n",
                "  * Chapter 6: Application to unsupervised machine learning\n",
                "\n",
                "  * Chapter 7: Application to optimal transport problems\n",
                "\n",
                "  * Chapter 8: Application to some partial differential equations\n",
                " \n",
                "## References \n",
                "\n",
                "Our primarily intention in this monograph is to provide an introduction to our Python library, and only a brief bibliography is included here, while we refer to the research papers cited below for additional references. Indeed, a large literature is available on support vector machines and reproducing kernel Hilbert spaces (RKHS), it is not our purpose to review it here. The interested reader can refer to the textbooks by Berlinet and Thomas-Agnan  \\cite{BerlinetThomasAgnan:2004} and Fasshauer \\cite{Fasshauer:2006},\\cite{Fasshauer:2007},\\cite{Fasshauer} which were very influential in the development of the present code and the reader will find therein a background on the subject.  \n",
                "\n",
                "Our own contributions concerning the kernel-based meshfree algorithms presented in this monograph can be found in the research papers by LeFloch and Mercier \\cite{LeFloch-Mercier:2015},\\cite{LeFloch-Mercier:2017},\\cite{LeFloch-Mercier:2020a},\\cite{LeFloch-Mercier:2020b},\\cite{LeFloch-Mercier:2021}.  Moreover, the unpublished notes  \\cite{LeFlochMercierMiryusupov:2021a}--\\cite{LeFlochMercierMiryusupov:2021f} contain earlier versions of the material in this monograph. For additional material on meshfree methods and kernel-based methods and applications in fluid and material dynamics, see for instance \n",
                "\\cite{BabuskaBanerjeeOsborn:2003},\\cite{BessaFosterBelytschkoLiu:2014},\\cite{GuntherLiu:1998},\\cite{HaghighatRaissibMoureGomezJuanes:2021},\\cite{KorzeniowskiWeinberg:2021},\\cite{LiLiu:2004},\\cite{Liu:2016},\\cite{Nakano:2017},\\cite{OhDavisJeong:2012},\\cite{SalehiDehghan:2013},\\cite{SirignanoSpiliopoulos:2018},\\cite{ZhouLi:2006}. \n",
                "\n",
                "\n",
                " \n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
