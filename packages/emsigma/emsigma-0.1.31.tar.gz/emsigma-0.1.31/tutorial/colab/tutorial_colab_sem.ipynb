{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link G-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install SIGMA using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTooJbLTkDWl"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "4362PEEZfpQB",
    "outputId": "3225a4d8-7859-46d9-e634-2d86d52c23e0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sigma\n",
    "from sigma.utils import normalisation as norm \n",
    "from sigma.utils import visualisation as visual\n",
    "from sigma.utils.load import SEMDataset\n",
    "from sigma.src.utils import same_seeds\n",
    "from sigma.src.dim_reduction import Experiment\n",
    "from sigma.models.autoencoder import AutoEncoder\n",
    "from sigma.src.segmentation import PixelSegmenter\n",
    "from sigma.gui import gui\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olPzlO4gsAuG"
   },
   "source": [
    "# Load files\n",
    "\n",
    "Load the *.bcf* file and create an object of `SEMDataset` (which uses hyperspy as backend.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wd0omy47rJb"
   },
   "outputs": [],
   "source": [
    "!gdown --id '1woNRlyrBbUDIClYp_HNldzA2evdpArsi' -O 'test.bcf'\n",
    "\n",
    "file_path = 'test.bcf'\n",
    "sem = SEMDataset(file_path)\n",
    "sem.set_feature_list(['Al_Ka', 'C_Ka', 'Ca_Ka', 'Fe_Ka', 'K_Ka', 'O_Ka', 'Si_Ka', 'Ti_Ka', 'Zn_La'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or upload file with GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "file_path=list(uploaded.keys())[0]\n",
    "sem = SEMDataset(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpqYK5B6m-3k"
   },
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## View the dataset\n",
    "\n",
    "**Use `gui.view_bcf_dataset(sem)`** to check the BSE image, the sum spectrum, and the elemental maps.<br>\n",
    "\n",
    "The default *feature list* (or elemental peaks) is extracted from the metadata of the .bcf file. The elemental peaks can be manually modified in `Feature list` and click `set` to set the new feature list. The elemental intensity maps will be generated when clicking the `set` button. In this way, the raw hyperspectral imaging (HSI) dataset has now become elemental maps with dimension of 279 x 514 x 9 (for the test file).\n",
    "\n",
    "**How can users determine the elements for analysis?**<br>\n",
    "Users can use the interactive widget in the tab \"EDX sum spectrum\" to check the energy peaks and determine the `Feature list` for further amalyses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRccG0O2plY_"
   },
   "outputs": [],
   "source": [
    "gui.view_bcf_dataset(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the GUI, we can view the dataset directly with the `sem` object:\n",
    "\n",
    "1. `sem.bse`: access the back-scattered electron (as a hyperspy `Signal2D` object).<br>\n",
    "2. `sem.edx`: access the edx dataset (as a hyperspy `EDSSEMSpectrum` object).<br>\n",
    "3. `visual.plot_sum_spectrum(sem.edx)`: view the sum spectrum (or use hyperspy built-in function `sem.edx.sum().plot(xray_lines=True)`).<br>\n",
    "4. `sem.feature_list`: view the default chosen elemental peaks in the edx dataset.<br>\n",
    "5. `sem.set_feature_list`: set new elemental peaks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjNVT8T9ttZx"
   },
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several (optional) functions to process the dataset:\n",
    "1. `sem.rebin_signal(size=(2,2))`: rebin the edx signal with the size of 2x2. After rebinning the dataset, we can access the binned edx or bse data using `sem.edx_bin` or `sem.bse_bin`. \n",
    "> Note: The size of binning may be changed depending on the number of pixels and signal-to-noise ratio of the EDS spectra. If the input HSI-EDS data contains too many pixels, that may crush the RAM. As for the signal-to-noise, the counts per pixel of the data is better to be higher than 100. In the case of the test dataset, the average counts per pixel is 29.94 before binning and will be 119.76 after binning.\n",
    "\n",
    "2. `sem.peak_intensity_normalisation()`: normalise the x-ray intensity along energy axis.\n",
    "\n",
    "3. `sem.remove_fist_peak(end=0.1)`: remove the first x-ray peak (most likely noise) by calling the function with the argument `end`. For example, if one wants to remove the intensity values from 0-0.1 keV, set `end=0.1`. This is an optional step.\n",
    "\n",
    "4. `visual.plot_intensity_maps`: Plot the elemental intensity maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kwTAEa5rz7r"
   },
   "outputs": [],
   "source": [
    "# Rebin both edx and bse dataset\n",
    "sem.rebin_signal(size=(2,2))\n",
    "\n",
    "# normalisation to make the spectrum of each pixel summing to 1.\n",
    "sem.peak_intensity_normalisation()\n",
    "\n",
    "# Remove the first peak until the energy of 0.1 keV\n",
    "sem.remove_fist_peak(end=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NARhLv5ASNry"
   },
   "outputs": [],
   "source": [
    "# View the dataset (bse, edx etc.) again to check differences. \n",
    "# Note that a new tab (including the binned elemental maps) will show up only if the user runs the sem.rebin_signal.\n",
    "\n",
    "gui.view_bcf_dataset(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing steps yield a HSI datacube with the dimension of 139 x 257 x 9 (due to the 2x2 binning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59GFs_0SBSkW"
   },
   "source": [
    "## Normalisation\n",
    "\n",
    "Before dimensionality reduction, we normalise the elemental maps use `sem.normalisation()`, where we can pass a list containing (optional) sequential normalisation steps.\n",
    "\n",
    "> Note that the pre-processing steps are all optional, but if the user opts for the softmax option, z-score  step should be applied beforehand, i.e., the softmax normalization procedure requires input data that is z-score normalized. The purpose of the combinatorial normalization step (z-score + softmax) is to provide the autoencoder with an input that includes global information from all pixels and ‘intentional bias’ within a single pixel. \n",
    "\n",
    ">`neighbour_averaging` is equivilant to apply a 3x3 mean filter to the HSI-EDS data and is an optional step. `zscore` rescales the intensity values within each elemental map such that the mean of all of the values is 0 and the standard deviation is 1 for each map. For example, after z-score normalisation, the Fe Ka map should contain pixels with intensity values that yield `mean=0` and `std=1`. `softmax` is applied within each individual pixel containing z-scores of different elemental peaks. For example, if 5 elemental maps are specified, the 5 corresponding z-scores for each individual pixel will be used to calculate the outputs of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgCJ-sfVsNbb"
   },
   "outputs": [],
   "source": [
    "# Normalise the dataset using the (optional) sequential three methods.\n",
    "sem.normalisation([norm.neighbour_averaging, \n",
    "                   norm.zscore, \n",
    "                   norm.softmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `gui.view_pixel_distributions` to view the intensity distributions after each sequential normalisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXRo2LvTqxbK"
   },
   "outputs": [],
   "source": [
    "gui.view_pixel_distributions(sem=sem, \n",
    "                             norm_list=[norm.neighbour_averaging,\n",
    "                                        norm.zscore,\n",
    "                                        norm.softmax], \n",
    "                             cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PaFMEOprt3B"
   },
   "source": [
    "## (Optional) Assign RGB to elemental peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZSQgdJsZTgV"
   },
   "outputs": [],
   "source": [
    "gui.view_rgb(sem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check elemental distribution after normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uns525mlsjfm"
   },
   "outputs": [],
   "source": [
    "print('After normalisation:')\n",
    "gui.view_intensity_maps(edx=sem.normalised_elemental_data, element_list=sem.feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2GcAoSlnGuN"
   },
   "source": [
    "# Dimensionality reduction: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_7bn6yJBjNo"
   },
   "source": [
    "## Initialise experiment / model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `Experiment`**<br>\n",
    "> `descriptor`: *str*. The name of the model. It will be used as the model name upon saving the model.<br>\n",
    "\n",
    "> `general_results_dir`: *path*. The folder path to save the model(the model will automatically save in the specified folder).<br>\n",
    "\n",
    "> `model`: *Autoencoder*. The model to be used for training. At this moment, only the vanilla autoencoder can be used. More models (e.g., variational autoencoder) will be implemented in the future versions.<br>\n",
    "\n",
    "> `model_args`: *Dict*. Keyword argument for the Autoencoder architecture. The most essential argument is'hidden_layer_sizes' which refers to number of hidden layers and corresponding neurons. For example, if *(512,256,128)*, the encoder will consist of three layers (the first leayer 512 neurons, the second 256 nwurons, and the third 128 neurons). The decoder will also be three layers (128 neurons, 256 nwurons, and 512 neurons). The default setting is recommonded in general cases. Increasing the numbers of layers and neurons will increase the complexity of the model, which raise the risk of overfitting.<br>\n",
    "\n",
    "> `chosen_dataset`: *np.ndarray*. Normalised HSI-EDS data. The size should be (width, height, number of elemental maps).<br>\n",
    "\n",
    "> `save_model_every_epoch`: *bool*. If 'True', the autoencoder model will be saved for each iteration. If 'False', the model will be save only when the loss value is lower than the recorded value.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUcL2sddrLHC",
    "outputId": "2bc7fd73-d250-40f8-c5b1-755e13058bcb"
   },
   "outputs": [],
   "source": [
    "# The integer in this function can determine different initialised parameters of model (tuning sudo randomness)\n",
    "# This can influence the result of dimensionality reduction and change the latent space.\n",
    "same_seeds(1)\n",
    "\n",
    "# set the folder path to save the model(the model will automatically save in the specified folder)\n",
    "result_folder_path='./' \n",
    "\n",
    "# Set up the experiment, e.g. determining the model structure, dataset for training etc.\n",
    "ex = Experiment(descriptor='softmax',\n",
    "                general_results_dir=result_folder_path,\n",
    "                model=AutoEncoder,\n",
    "                model_args={'hidden_layer_sizes':(512,256,128)}, \n",
    "                chosen_dataset=sem.normalised_elemental_data,\n",
    "                save_model_every_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEgWJf45Bp4F"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `ex.run_model`**<br>\n",
    "> `num_epochs`: *int*. The number of entire passes through the training dataset. 50-100 is recommonded for this value. A rule of thumb is that if the loss value stops reducing, that epoch my be a good point to stop. <br>\n",
    "\n",
    "> `batch_size`: *int*. The number of data points per gradient update. Values between 32-128 are recommended. smaller batch size means more updates within an epoch, but is more stochastic for the optimisation process.<br>\n",
    "\n",
    "> `learning_rate`: *float* in a range between 0-1. The learning rate controls how quickly the model is adapted to the problem. 1e-4 is recommended. Higher learning rate may yield faster convergence but have a risk to be stuck in an undesirable local minima.<br>\n",
    "\n",
    "> `task`: *str*. if 'train_all', all data points will be used for training the autoencoder. If 'train_eval', training will be conducted on the training set (85% dataset) and testing on a testing set (15%) for evaluation. The recommended procedure is to run the 'train_eval' for hyperparameter selection, and 'train_all' for the final analysis.<br>\n",
    "\n",
    "> `criterion`: *str*. If 'MSE', the criterion is to measure the mean squared error (squared L2 norm) between each element in the input x and target y. 'MSE' is the only option. Other criteria will be implemented in the future versions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgajwhO3rNv1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ex.run_model(num_epochs=50,\n",
    "             batch_size=64,\n",
    "             learning_rate=1e-4, \n",
    "             weight_decay=0.0, \n",
    "             task='train_all', \n",
    "             criterion='MSE'\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pd5l795DnL8r"
   },
   "source": [
    "# Pixel segmentation: Gaussian mixture modelling (GMM) clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXi8fj7mBsxt"
   },
   "source": [
    "## (Optional) Load pre-trained Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3l8O-UwrjcxP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = './' # model path (the model path should be stored in the folder 'result_folder_path')\n",
    "ex.load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_qEVkRAV7BL"
   },
   "source": [
    "## Measure Baysian information criterion (BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gui.view_bic` iteratively calculates the BIC for Gaussian mixture models using the number of Gaussian components.\n",
    "\n",
    "**Parameters for `gui.view_bic`**<br>\n",
    "> `latent`: *np.ndarray*. 2D representations learned by the autoencoder. The size of the input array must be (n, 2), where n is number of total data points.<br>\n",
    "\n",
    "> `model`: *str*. Model for calculation of BIC. Only 'GaussianMixture' is available for now.<br>\n",
    "\n",
    "> `n_components`: *int*. If `n_components=20`, it shows the BIC values for GMM using `n_components` from 1 to 20.<br>\n",
    "\n",
    "> `model_args`: *Dict*. Keyword arguments for the GMM model in sklearn. For example, `random_state` is to specify the random seed for optimisation (This can make the results reproduciable); `init_params` is to specify the parameter initialisation of the GMM model ('kmeans' is recommended). See mode detail [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP2TyaUrL-D0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent = ex.get_latent()\n",
    "gui.view_bic(latent=latent,\n",
    "             model='GaussianMixture'\n",
    "             n_components=20,\n",
    "             model_args={'random_state':6, 'init_params':'kmeans'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63qxth1gYc8v"
   },
   "source": [
    "## Run GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `PixelSegmenter`**<br>\n",
    "> `latent`: *np.ndarray*. The size of the input array must be (n, 2), where n is number of total data points.<br>\n",
    "\n",
    "> `dataset_norm`: *np.ndarray*. Normalised HSI-EDS data. The size should be *(width, height, number of elemental maps)*. <br>\n",
    "\n",
    "> `sem`: *SEMDataset*. The SEM object created in the beginning.<br>\n",
    "\n",
    "> `method`: *str*. Model for clustering.<br>\n",
    "\n",
    "> `method_args`: *Dict*. Keyword arguments for the GMM model in sklearn. [See mode detail here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyU0I3--rTgM"
   },
   "outputs": [],
   "source": [
    "latent = ex.get_latent()\n",
    "ps = PixelSegmenter(latent=latent, \n",
    "                    dataset_norm=sem.normalised_elemental_data, \n",
    "                    sem=sem,\n",
    "                    method='GaussianMixture',\n",
    "                    method_args={'n_components':12, 'random_state':6, 'init_params':'kmeans'} )\n",
    "                    # can change random_state to different integer i.e. 10 or 0 to adjust the clustering result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyVkwD-kwFIF"
   },
   "source": [
    "## Checking latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `gui.view_latent_space`**<br>\n",
    "> `ps`: *PixelSegmenter*. The object of PixelSegmetor which is just created.<br>\n",
    "\n",
    "> `color`: *bool*. If `True`, the the latent space will be colour-coded based on their cluster labels.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjrxxOLFrVfu"
   },
   "outputs": [],
   "source": [
    "# Plot latent sapce (2-dimensional) with corresponding Gaussian models\n",
    "gui.view_latent_space(ps=ps, color=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `gui.check_latent_space`**<br>\n",
    "> `ps`: *PixelSegmenter*. The object of PixelSegmetor which is just created.<br>\n",
    "\n",
    "> `ratio_to_be_shown`: *float*. The value must be between 0-1. For example, if 0.5, the latent space will only show 50% data points.<br>\n",
    "\n",
    "> `show_map`: *bool*. If `True`, the corresponding locations of the data points will be overlaid on the BSE image.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKJ_bEDorXpp"
   },
   "outputs": [],
   "source": [
    "# visualise the latent space\n",
    "gui.check_latent_space(ps=ps,ratio_to_be_shown=0.5, show_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `gui.check_latent_space`**<br>\n",
    "> `ps`: *PixelSegmenter*. The object of PixelSegmetor which is just created.<br>\n",
    "\n",
    "> `bins`: *int*. Number of bins for the given interval of the latent space.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47TKUHLorZyV"
   },
   "outputs": [],
   "source": [
    "# check the density of latent space\n",
    "gui.plot_latent_density(ps=ps, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uFv-qDOLFye"
   },
   "source": [
    "## Checking each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0fAz4OErbxj"
   },
   "outputs": [],
   "source": [
    "# ps.set_feature_list(['Al_Ka', 'C_Ka', 'Ca_Ka', 'Fe_Ka', 'K_Ka', 'O_Ka', 'Si_Ka', 'Ti_Ka', 'Zn_La'])\n",
    "gui.show_cluster_distribution(ps=ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7hCyeiKLLii"
   },
   "source": [
    "## Checking cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH9196CXre1j"
   },
   "outputs": [],
   "source": [
    "# Plot phase map using the corresponding GM model\n",
    "gui.view_phase_map(ps=ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `gui.view_clusters_sum_spectra`**<br>\n",
    "\n",
    "> `ps`: *PixelSegmenter*. The object of PixelSegmetor which is just created.<br>\n",
    "\n",
    "> `normalisation`: *bool*. If `True`, the sum spectra will be normalised.<br>\n",
    "\n",
    "> `spectra_range`: *tuple*. Set the limits of the energy axes (in *KeV*).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yizWZ7rrgvx"
   },
   "outputs": [],
   "source": [
    "gui.view_clusters_sum_spectra(ps=ps, normalisation=True, spectra_range=(0,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDqkqAHpLaN0"
   },
   "source": [
    "# Unmixing cluster spectrums using Non-negative Matrix Fatorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for `gui.get_unmixed_edx_profile`**<br>\n",
    "> `clusters_to_be_calculated`: *str* or *List*. If `'All'`, all cluster-spectra will be included in the data matrix for NMF. If it is a list of integers (e.g. [0,1,2,3]), only #0, #1, #2, and #3 cluster-spectra will be used as the data matrix for NMF.<br>\n",
    "\n",
    "> `normalised`: *bool*. If `True`, the sum spectra will be normalised before NMF unmixing.<br>\n",
    "\n",
    "> `method`: *str*. Model to be used.<br>\n",
    "\n",
    "> `method_args`: *Dict*. Keyword arugments for the NMF method (or others). [See more detail here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWeg9UomrmzX"
   },
   "outputs": [],
   "source": [
    "weights, components = ps.get_unmixed_edx_profile(clusters_to_be_calculated='All', \n",
    "                                                 n_components='All',\n",
    "                                                 normalised=False, \n",
    "                                                 method='NMF', \n",
    "                                                 method_args={'init':'nndsvd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me57SkqNrpSg"
   },
   "outputs": [],
   "source": [
    "gui.show_unmixed_weights_and_compoments(ps=ps, weights=weights, components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check abundance map for components (using RGB maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gui.show_abundance_map(ps=ps, weights=weights, components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSIhNd4oLTHr",
    "tags": []
   },
   "source": [
    "# Statistics infro from clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cermaBpurkg7"
   },
   "outputs": [],
   "source": [
    "gui.show_cluster_stats(ps=ps)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Tutorial_local",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
